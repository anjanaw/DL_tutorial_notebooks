{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjn5N1OB35KzwpDOttbIMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/DepthAnyVideo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth ANy Video:<br>\n",
        "https://github.com/Nightmare-n/DepthAnyVideo"
      ],
      "metadata": {
        "id": "Ve03Tv80g-ej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkOF0dlzGrdt",
        "outputId": "ec74cff8-a4ef-4a71-c84b-6d9ba16b96c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DepthAnyVideo'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 62 (delta 15), reused 4 (delta 4), pack-reused 38 (from 1)\u001b[K\n",
            "Receiving objects: 100% (62/62), 14.68 MiB | 17.24 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "/content/DepthAnyVideo\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nightmare-n/DepthAnyVideo\n",
        "%cd DepthAnyVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DepthAnyVideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5szrgEprHxqK",
        "outputId": "1f3a16e9-5cf6-4181-ba4c-3f0f8d1bccad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DepthAnyVideo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_infer.py --data_path ./demos/arch_2.jpg --output_dir ./outputs/ --max_resolution 320"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhEQHfWUH1pG",
        "outputId": "78fea704-0304-4241-e50c-50c3b340774f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "2024-12-30 16:41:21.858084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-30 16:41:21.877915: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-30 16:41:21.883745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-30 16:41:21.898624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-30 16:41:23.083761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:root:output dir = ./outputs/\n",
            "vae/config.json: 100% 561/561 [00:00<00:00, 3.22MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 391M/391M [00:09<00:00, 42.8MB/s]\n",
            "scheduler/scheduler_config.json: 100% 274/274 [00:00<00:00, 1.93MB/s]\n",
            "unet/config.json: 100% 834/834 [00:00<00:00, 6.83MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.69G/5.69G [02:15<00:00, 42.1MB/s]\n",
            "unet_interp/config.json: 100% 835/835 [00:00<00:00, 6.34MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.69G/5.69G [02:15<00:00, 42.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_infer.py --data_path ./demos/wooly_mammoth.mp4 --output_dir ./outputs/ --max_resolution 64"
      ],
      "metadata": {
        "id": "OTcvdFFqK7TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "from easydict import EasyDict\n",
        "import numpy as np\n",
        "import torch\n",
        "from dav.pipelines import DAVPipeline\n",
        "from dav.models import UNetSpatioTemporalRopeConditionModel\n",
        "from diffusers import AutoencoderKLTemporalDecoder, FlowMatchEulerDiscreteScheduler\n",
        "from dav.utils import img_utils\n",
        "\n",
        "\n",
        "def seed_all(seed: int = 0):\n",
        "    \"\"\"\n",
        "    Set random seeds of all components.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "if \"__main__\" == __name__:\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Run video depth estimation using Depth Any Video.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model_base\",\n",
        "        type=str,\n",
        "        default=\"hhyangcs/depth-any-video\",\n",
        "        help=\"Checkpoint path or hub name.\",\n",
        "    )\n",
        "\n",
        "    # data setting\n",
        "    parser.add_argument(\n",
        "        \"--data_path\", type=str, required=False, help=\"input data directory.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, required=False, help=\"Output directory.\"\n",
        "    )\n",
        "\n",
        "    # inference setting\n",
        "    parser.add_argument(\n",
        "        \"--denoise_steps\",\n",
        "        type=int,\n",
        "        default=3,\n",
        "        help=\"Denoising steps, 1-3 steps work fine.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_frames\",\n",
        "        type=int,\n",
        "        default=32,\n",
        "        help=\"Number of frames to infer per forward\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--decode_chunk_size\",\n",
        "        type=int,\n",
        "        default=16,\n",
        "        help=\"Number of frames to decode per forward\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_interp_frames\",\n",
        "        type=int,\n",
        "        default=16,\n",
        "        help=\"Number of frames for inpaint inference\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_overlap_frames\",\n",
        "        type=int,\n",
        "        default=6,\n",
        "        help=\"Number of frames to overlap between windows\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_resolution\",\n",
        "        type=int,\n",
        "        default=1024,  # decrease for faster inference and lower memory usage\n",
        "        help=\"Maximum resolution for inference.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=None, help=\"Random seed.\")\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args.data_path = \"./demos/wooly_mammoth.mp4\"\n",
        "    args.output_dir = \"./outputs/\"\n",
        "    args.max_resolution = 64\n",
        "    cfg = EasyDict(vars(args))\n",
        "\n",
        "    if cfg.seed is None:\n",
        "        import time\n",
        "\n",
        "        cfg.seed = int(time.time())\n",
        "    seed_all(cfg.seed)\n",
        "\n",
        "    device_type = \"cuda\"\n",
        "    device = torch.device(device_type)\n",
        "\n",
        "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "    logging.info(f\"output dir = {cfg.output_dir}\")\n",
        "\n",
        "    vae = AutoencoderKLTemporalDecoder.from_pretrained(cfg.model_base, subfolder=\"vae\")\n",
        "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
        "        cfg.model_base, subfolder=\"scheduler\"\n",
        "    )\n",
        "    vae.half()\n",
        "\n",
        "    unet = UNetSpatioTemporalRopeConditionModel.from_pretrained(\n",
        "        cfg.model_base, subfolder=\"unet\"\n",
        "    )\n",
        "    unet.half()\n",
        "\n",
        "    unet_interp = UNetSpatioTemporalRopeConditionModel.from_pretrained(\n",
        "        cfg.model_base, subfolder=\"unet_interp\"\n",
        "    )\n",
        "    unet_interp.half()\n",
        "\n",
        "\n",
        "    pipe = DAVPipeline(\n",
        "        vae=vae,\n",
        "        unet=unet,\n",
        "        unet_interp=unet_interp,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    file_name = cfg.data_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    is_video = cfg.data_path.endswith(\".mp4\")\n",
        "    if is_video:\n",
        "        num_interp_frames = cfg.num_interp_frames\n",
        "        num_overlap_frames = cfg.num_overlap_frames\n",
        "        num_frames = cfg.num_frames\n",
        "        assert num_frames % 2 == 0, \"num_frames should be even.\"\n",
        "        assert (\n",
        "            2 <= num_overlap_frames <= (num_interp_frames + 2 + 1) // 2\n",
        "        ), \"Invalid frame overlap.\"\n",
        "        max_frames = (num_interp_frames + 2 - num_overlap_frames) * (num_frames // 2)\n",
        "        image, fps = img_utils.read_video(cfg.data_path, max_frames=max_frames)\n",
        "    else:\n",
        "        image = img_utils.read_image(cfg.data_path)\n",
        "\n",
        "    image = img_utils.imresize_max(image, cfg.max_resolution)\n",
        "    image = img_utils.imcrop_multi(image)\n",
        "    image_tensor = np.ascontiguousarray(\n",
        "        [_img.transpose(2, 0, 1) / 255.0 for _img in image]\n",
        "    )\n",
        "    image_tensor = torch.from_numpy(image_tensor).to(device)\n",
        "    print('inputs:', image_tensor.shape)\n",
        "    print('cfg.num_overlap_frames:', cfg.num_overlap_frames)\n",
        "    print('cfg.num_interp_frames:', cfg.num_interp_frames)\n",
        "    print('cfg.decode_chunk_size:', cfg.decode_chunk_size)\n",
        "    print('cfg.denoise_steps:', cfg.denoise_steps)\n",
        "\n",
        "    with torch.no_grad(), torch.autocast(device_type=device_type, dtype=torch.float16):\n",
        "        pipe_out = pipe(\n",
        "            image_tensor,\n",
        "            num_frames=cfg.num_frames,\n",
        "            num_overlap_frames=cfg.num_overlap_frames,\n",
        "            num_interp_frames=cfg.num_interp_frames,\n",
        "            decode_chunk_size=cfg.decode_chunk_size,\n",
        "            num_inference_steps=cfg.denoise_steps,\n",
        "        )\n",
        "\n",
        "    disparity = pipe_out.disparity\n",
        "    disparity_colored = pipe_out.disparity_colored\n",
        "    image = pipe_out.image\n",
        "    print('out_depth:', image.shape)\n",
        "    # (N, H, 2 * W, 3)\n",
        "    merged = np.concatenate(\n",
        "        [\n",
        "            image,\n",
        "            disparity_colored,\n",
        "        ],\n",
        "        axis=2,\n",
        "    )\n",
        "    print('out_depth merged:', merged.shape)\n",
        "\n",
        "    if is_video:\n",
        "        img_utils.write_video(\n",
        "            os.path.join(cfg.output_dir, f\"{file_name}.mp4\"),\n",
        "            merged,\n",
        "            fps,\n",
        "        )\n",
        "    else:\n",
        "        img_utils.write_image(\n",
        "            os.path.join(cfg.output_dir, f\"{file_name}.png\"),\n",
        "            merged[0],\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEgW6wAodwoa",
        "outputId": "ad0244f2-d06d-4883-dd58-21bfe6af9cc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: torch.Size([192, 3, 32, 64])\n",
            "cfg.num_overlap_frames: 6\n",
            "cfg.num_interp_frames: 16\n",
            "cfg.decode_chunk_size: 16\n",
            "cfg.denoise_steps: 3\n",
            "rgb_latent: torch.Size([1, 30, 4, 4, 8])\n",
            "latent_model_input: torch.Size([1, 30, 8, 4, 8])\n",
            "latent_model_input: torch.Size([1, 30, 8, 4, 8])\n",
            "latent_model_input: torch.Size([1, 30, 8, 4, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:06<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_depth: (184, 32, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('out_depth merged:', merged.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5kx_38EvC8j",
        "outputId": "3d84f4a7-ccc8-4d22-d709-13dd28c1b6c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_depth merged: (184, 32, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "192/6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0KtolfUmPV3",
        "outputId": "ad3cbe10-48c5-4c29-982f-f9be26eaa3b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}