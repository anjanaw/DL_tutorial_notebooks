{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpYpTrkBLZq7ke8+uR13DK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/GPT2_LLaMA3_2_Prompt_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image encoder: 1, 256, 32, 32, 32\n",
        "prompt encoder: 1, 256, 32, 32, 32"
      ],
      "metadata": {
        "id": "8uDteAqeq4jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT-2 to Extract Features"
      ],
      "metadata": {
        "id": "STzZitCduC1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3enct_Oljvel",
        "outputId": "58176b6d-c5cf-4147-eb8a-2cd635298f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pool Feature shape: torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "def extract_text_features(prompt, model_name=\"gpt2\"):\n",
        "    # Load GPT-2 tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2Model.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize and convert to tensor\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Extract hidden states (text embeddings)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get last hidden state (shape: [batch_size, sequence_length, hidden_dim])\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Optionally, pool embeddings (mean of sequence)\n",
        "    pooled_features = hidden_states.mean(dim=1)\n",
        "\n",
        "    return pooled_features\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Segment the pituitary tumor?\"\n",
        "features = extract_text_features(prompt)\n",
        "\n",
        "# Print the shape of the extracted features\n",
        "print(f\"Pool Feature shape: {features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLaMA 3.2 to Extract Features"
      ],
      "metadata": {
        "id": "DWUHJg9PuMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def extract_text_features(prompt, model_name=\"meta-llama/llama-3.2-1b\"):\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Extract hidden states (text embeddings)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get last hidden state (shape: [batch_size, sequence_length, hidden_dim])\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Optionally pool the embeddings (mean of sequence)\n",
        "    pooled_features = hidden_states.mean(dim=1)\n",
        "\n",
        "    return pooled_features\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Segment the pituitary tumor?\"\n",
        "features = extract_text_features(prompt)\n",
        "\n",
        "# Print the shape of the extracted features\n",
        "print(f\"Feature shape: {features.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z0s3e4AkGsh",
        "outputId": "9efcf313-c5be-4093-b712-e3577ebd277b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: torch.Size([1, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4A4X-ZWGluPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}