{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmIucDOsVB71vX0s2qlH+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb5a82396238424d818af679ee59eaa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6d227943c9349ef896e96ba58aef4f2",
              "IPY_MODEL_b486a657adba4a10b27ea5263763d9f6",
              "IPY_MODEL_6ac614b7877447e1964cc81e5e8ce89b"
            ],
            "layout": "IPY_MODEL_ff7bf10209ce41df854afbe66fb46ab0"
          }
        },
        "f6d227943c9349ef896e96ba58aef4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5b296201064ad990b7d62f0a54aff1",
            "placeholder": "​",
            "style": "IPY_MODEL_f797545c878b41a4bfba48c289e7bf69",
            "value": "Map: 100%"
          }
        },
        "b486a657adba4a10b27ea5263763d9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6c5e18322448d9b9ed469c480082c9",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57b80b97776b4f9e932856756f05f7ca",
            "value": 5
          }
        },
        "6ac614b7877447e1964cc81e5e8ce89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ac68e0ffc84f7f9f7ffbcaa004d6b9",
            "placeholder": "​",
            "style": "IPY_MODEL_958da15be4d6482b9bb81d4615ca50e2",
            "value": " 5/5 [00:00&lt;00:00, 69.95 examples/s]"
          }
        },
        "ff7bf10209ce41df854afbe66fb46ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5b296201064ad990b7d62f0a54aff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f797545c878b41a4bfba48c289e7bf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a6c5e18322448d9b9ed469c480082c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b80b97776b4f9e932856756f05f7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08ac68e0ffc84f7f9f7ffbcaa004d6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958da15be4d6482b9bb81d4615ca50e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f2f6441c51147ff99029d1301e1d596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_278895c99bc3421db701314b10b5d263",
              "IPY_MODEL_3753bd61354142c59d2095ed96fcf58d",
              "IPY_MODEL_db14a44779614d37b14d655f30e14a12"
            ],
            "layout": "IPY_MODEL_def9ddf90a704cff8b682c01cea190da"
          }
        },
        "278895c99bc3421db701314b10b5d263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08da70448874baaa33ba89f6e6098ea",
            "placeholder": "​",
            "style": "IPY_MODEL_c5315bb1b9f44595b781fcb6fae0921d",
            "value": "Map: 100%"
          }
        },
        "3753bd61354142c59d2095ed96fcf58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b00bfcb9544ed8befe9fd8d04593fd",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cca90e7e2fc4c678ab5d7b44af634e6",
            "value": 5
          }
        },
        "db14a44779614d37b14d655f30e14a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002379d98d7149d1908f58af172b5fb2",
            "placeholder": "​",
            "style": "IPY_MODEL_d62b1b3db6af4175befe759c584425d9",
            "value": " 5/5 [00:00&lt;00:00, 86.97 examples/s]"
          }
        },
        "def9ddf90a704cff8b682c01cea190da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08da70448874baaa33ba89f6e6098ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5315bb1b9f44595b781fcb6fae0921d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64b00bfcb9544ed8befe9fd8d04593fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cca90e7e2fc4c678ab5d7b44af634e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "002379d98d7149d1908f58af172b5fb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d62b1b3db6af4175befe759c584425d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/LLM_GPT2_Entropy_Semantic_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuZDlbTR87aD",
        "outputId": "3d8eade2-5dd9-4c64-f441-9712e62602fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/484.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/484.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2_FFT"
      ],
      "metadata": {
        "id": "hvUR4NWdBh7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from datasets import Dataset\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Seed setting function\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 50\n",
        "set_seed(seed)\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if necessary\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare 5 QA samples for training and validation\n",
        "train_qa_samples = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
        "    {\"question\": \"What is the largest planet?\", \"answer\": \"The largest planet is Jupiter.\"},\n",
        "    {\"question\": \"Who painted the Mona Lisa?\", \"answer\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
        "    {\"question\": \"What is the speed of light?\", \"answer\": \"The speed of light is approximately 299,792 kilometers per second.\"}\n",
        "]\n",
        "\n",
        "valid_qa_samples = [\n",
        "    {\"question\": \"Which city is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Can you tell me who authored '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
        "    {\"question\": \"What planet is the biggest in our solar system?\", \"answer\": \"The largest planet is Jupiter.\"},\n",
        "    {\"question\": \"Who is the artist behind the Mona Lisa?\", \"answer\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
        "    {\"question\": \"How fast does light travel?\", \"answer\": \"The speed of light is approximately 299,792 kilometers per second.\"}\n",
        "]\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    input_text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=60)\n",
        "\n",
        "    # Clone input_ids into labels\n",
        "    labels = inputs[\"input_ids\"].copy()\n",
        "\n",
        "    # Mask question tokens and padding tokens in labels\n",
        "    question_length = len(tokenizer(f\"Question: {example['question']}\\nAnswer:\")[\"input_ids\"]) - 1\n",
        "    for i in range(len(labels)):\n",
        "        if i < question_length or labels[i] == tokenizer.pad_token_id:\n",
        "            labels[i] = tokenizer.eos_token_id  # Ignore question and padding tokens\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Convert samples to dataset and preprocess\n",
        "dataset_train = Dataset.from_list(train_qa_samples).map(preprocess_data, remove_columns=[\"question\", \"answer\"])\n",
        "dataset_valid = Dataset.from_list(valid_qa_samples).map(preprocess_data, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "# Convert to PyTorch DataLoader\n",
        "batch_size = 2\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch])\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch])\n",
        "    return input_ids, attention_mask, labels\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Cross-entropy loss function ignoring padding tokens\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# Function to save the best model based on validation loss\n",
        "def save_best_model(model, tokenizer, epoch, best_loss, current_loss, save_path=\"./gpt2-qa-best-loss-cml\"):\n",
        "    if current_loss < best_loss:\n",
        "        best_loss = current_loss\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "        print(f\"Best model saved at epoch {epoch} with validation loss: {best_loss:.4f}\")\n",
        "    return best_loss\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, num_epochs=10):\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Shift labels and logits for proper alignment\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            # Flatten logits and labels for loss calculation\n",
        "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(shift_logits, shift_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = validate(model, valid_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        best_val_loss = save_best_model(model, tokenizer, epoch + 1, best_val_loss, avg_val_loss)\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Shift labels and logits for proper alignment\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            # Flatten logits and labels for loss calculation\n",
        "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(shift_logits, shift_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Start training\n",
        "train(model, train_loader, valid_loader, optimizer, criterion, num_epochs=10)\n",
        "\n",
        "# Load the best fine-tuned model for inference\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Inference function\n",
        "def generate_answer(question):\n",
        "    model.eval()\n",
        "    input_text = f\"Question: {question} Answer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=60).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Example inference\n",
        "question = \"What is the capital of France?\"\n",
        "answer = generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "eb5a82396238424d818af679ee59eaa1",
            "f6d227943c9349ef896e96ba58aef4f2",
            "b486a657adba4a10b27ea5263763d9f6",
            "6ac614b7877447e1964cc81e5e8ce89b",
            "ff7bf10209ce41df854afbe66fb46ab0",
            "dd5b296201064ad990b7d62f0a54aff1",
            "f797545c878b41a4bfba48c289e7bf69",
            "5a6c5e18322448d9b9ed469c480082c9",
            "57b80b97776b4f9e932856756f05f7ca",
            "08ac68e0ffc84f7f9f7ffbcaa004d6b9",
            "958da15be4d6482b9bb81d4615ca50e2",
            "5f2f6441c51147ff99029d1301e1d596",
            "278895c99bc3421db701314b10b5d263",
            "3753bd61354142c59d2095ed96fcf58d",
            "db14a44779614d37b14d655f30e14a12",
            "def9ddf90a704cff8b682c01cea190da",
            "f08da70448874baaa33ba89f6e6098ea",
            "c5315bb1b9f44595b781fcb6fae0921d",
            "64b00bfcb9544ed8befe9fd8d04593fd",
            "8cca90e7e2fc4c678ab5d7b44af634e6",
            "002379d98d7149d1908f58af172b5fb2",
            "d62b1b3db6af4175befe759c584425d9"
          ]
        },
        "id": "GgBYaFhP9D9p",
        "outputId": "acd9ca84-4989-4946-b20d-48cb67b4bf50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb5a82396238424d818af679ee59eaa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f2f6441c51147ff99029d1301e1d596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 4.4203, Validation Loss: 1.5726\n",
            "Best model saved at epoch 1 with validation loss: 1.5726\n",
            "Epoch 2/10, Training Loss: 0.9883, Validation Loss: 0.9799\n",
            "Best model saved at epoch 2 with validation loss: 0.9799\n",
            "Epoch 3/10, Training Loss: 0.7012, Validation Loss: 0.6840\n",
            "Best model saved at epoch 3 with validation loss: 0.6840\n",
            "Epoch 4/10, Training Loss: 0.4719, Validation Loss: 0.4885\n",
            "Best model saved at epoch 4 with validation loss: 0.4885\n",
            "Epoch 5/10, Training Loss: 0.2782, Validation Loss: 0.4139\n",
            "Best model saved at epoch 5 with validation loss: 0.4139\n",
            "Epoch 6/10, Training Loss: 0.2091, Validation Loss: 0.3706\n",
            "Best model saved at epoch 6 with validation loss: 0.3706\n",
            "Epoch 7/10, Training Loss: 0.1939, Validation Loss: 0.3273\n",
            "Best model saved at epoch 7 with validation loss: 0.3273\n",
            "Epoch 8/10, Training Loss: 0.1228, Validation Loss: 0.2700\n",
            "Best model saved at epoch 8 with validation loss: 0.2700\n",
            "Epoch 9/10, Training Loss: 0.1057, Validation Loss: 0.2148\n",
            "Best model saved at epoch 9 with validation loss: 0.2148\n",
            "Epoch 10/10, Training Loss: 0.0695, Validation Loss: 0.1682\n",
            "Best model saved at epoch 10 with validation loss: 0.1682\n",
            "Q: What is the capital of France?\n",
            "A: The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Entropy"
      ],
      "metadata": {
        "id": "M95j44dctFYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from datasets import Dataset\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Inference function with entropy-based uncertainty calculation\n",
        "def generate_answer_with_entropy(question):\n",
        "    model.eval()\n",
        "    input_text = f\"Question: {question} Answer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=60).to(device)\n",
        "\n",
        "    # Generate with output scores to get logits at each generation step\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    # Calculate entropy for each generated token (from the output scores)\n",
        "    entropies = []\n",
        "    for score in output.scores:\n",
        "        # score shape: [batch_size, vocab_size]\n",
        "        probs = F.softmax(score, dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)  # entropy for each sample in the batch\n",
        "        entropies.append(entropy)\n",
        "    # Average entropy over generated tokens\n",
        "    mean_entropy = torch.mean(torch.stack(entropies))\n",
        "\n",
        "    # Decode generated text and extract answer part\n",
        "    answer = tokenizer.decode(output.sequences[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
        "    return answer, mean_entropy.item()\n",
        "\n",
        "# Example inference\n",
        "question = \"What is the capital of France?\"\n",
        "answer, uncertainty = generate_answer_with_entropy(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\\nMean Entropy (Uncertainty): {uncertainty:.4f}\")\n"
      ],
      "metadata": {
        "id": "P0Zx5Vz0q_xZ",
        "outputId": "e71cf0da-e8fe-4578-aa19-e74328acfb46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the capital of France?\n",
            "A: The capital of France is Paris.\n",
            "Mean Entropy (Uncertainty): 0.7557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Entropy with additional forward pass"
      ],
      "metadata": {
        "id": "5bG7WE0btIUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer_with_semantic_entropy(question, n_clusters=5):\n",
        "    model.eval()\n",
        "    input_text = f\"Question: {question} Answer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=60).to(device)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Generate output (without hidden states, as they might not include the generated tokens)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    generated_ids = output.sequences  # shape: [batch_size, sequence_length]\n",
        "    generated_sequence_length = generated_ids.shape[1]\n",
        "    print(f\"Input length: {input_length}, Generated sequence length: {generated_sequence_length}\")\n",
        "\n",
        "    # Decode full generated text\n",
        "    full_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    answer = full_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Now, run a forward pass on the entire generated sequence to get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(generated_ids, output_hidden_states=True)\n",
        "\n",
        "    # Extract the hidden states from the last layer: shape [batch_size, sequence_length, hidden_dim]\n",
        "    last_hidden = outputs.hidden_states[-1]\n",
        "\n",
        "    # Get the hidden states corresponding to the tokens generated after the prompt\n",
        "    generated_hidden = last_hidden[0, input_length:, :]\n",
        "    num_tokens = generated_hidden.shape[0]\n",
        "\n",
        "    if num_tokens == 0:\n",
        "        return answer, 0.0\n",
        "\n",
        "    # Convert hidden states to numpy array and perform clustering\n",
        "    generated_hidden_np = generated_hidden.cpu().numpy()\n",
        "    k = min(n_clusters, num_tokens)\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(generated_hidden_np)\n",
        "\n",
        "    # Compute probability distribution over clusters\n",
        "    counts = np.bincount(cluster_labels, minlength=k)\n",
        "    probs = counts / np.sum(counts)\n",
        "\n",
        "    # Compute semantic entropy\n",
        "    epsilon = 1e-10\n",
        "    semantic_entropy = -np.sum(probs * np.log(probs + epsilon))\n",
        "\n",
        "    return answer, semantic_entropy\n",
        "\n",
        "# Example usage:\n",
        "question = \"What is the capital of France?\"\n",
        "answer, sem_entropy = generate_answer_with_semantic_entropy(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\\nSemantic Entropy: {sem_entropy:.4f}\")\n"
      ],
      "metadata": {
        "id": "wNwT1sf4uXEx",
        "outputId": "9c07c196-e825-4669-a455-0f641863a1b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input length: 11, Generated sequence length: 19\n",
            "Q: What is the capital of France?\n",
            "A: The capital of France is Paris.\n",
            "Semantic Entropy: 1.4942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Entropy with Custom Generation Function"
      ],
      "metadata": {
        "id": "9t9pZUDatSuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def custom_generate(input_text, max_new_tokens=50, n_clusters=5):\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=60).to(device)\n",
        "    input_ids = inputs['input_ids']  # shape: [1, seq_length]\n",
        "\n",
        "    # List to store the hidden states for the last token at each generation step\n",
        "    collected_hidden_states = []  # Each element is a list of per-layer hidden states\n",
        "\n",
        "    # Initialize generated_ids with the input_ids\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Disable caching to recompute full hidden states for the entire sequence\n",
        "            outputs = model(generated_ids, output_hidden_states=True, use_cache=False)\n",
        "            # outputs.hidden_states is a tuple of length (n_layers + 1)\n",
        "            # Extract the hidden states for the last token from each layer\n",
        "            step_hidden = [layer_hidden[0, -1, :].unsqueeze(0) for layer_hidden in outputs.hidden_states]\n",
        "            collected_hidden_states.append(step_hidden)\n",
        "\n",
        "            # Greedy sampling for the next token\n",
        "            next_token_logits = outputs.logits[:, -1, :]  # shape: [1, vocab_size]\n",
        "            next_token_id = next_token_logits.argmax(dim=-1, keepdim=True)  # shape: [1, 1]\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the full generated sequence\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # For semantic entropy, use the hidden states from the final layer for each generated token\n",
        "    # Each element in collected_hidden_states is a list of per-layer representations;\n",
        "    # We take the last element (final layer) from each generation step.\n",
        "    last_layer_hidden_states = [step[-1] for step in collected_hidden_states]  # list of [1, hidden_dim]\n",
        "    if len(last_layer_hidden_states) == 0:\n",
        "        semantic_entropy = 0.0\n",
        "    else:\n",
        "        # Stack into a tensor of shape [num_generated_tokens, hidden_dim]\n",
        "        hidden_tensor = torch.cat(last_layer_hidden_states, dim=0)\n",
        "        num_tokens = hidden_tensor.shape[0]\n",
        "\n",
        "        # Convert to numpy for clustering\n",
        "        hidden_np = hidden_tensor.cpu().numpy()\n",
        "        k = min(n_clusters, num_tokens)\n",
        "        if k == 0:\n",
        "            semantic_entropy = 0.0\n",
        "        else:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "            cluster_labels = kmeans.fit_predict(hidden_np)\n",
        "            counts = np.bincount(cluster_labels, minlength=k)\n",
        "            probs = counts / np.sum(counts)\n",
        "            epsilon = 1e-10\n",
        "            semantic_entropy = -np.sum(probs * np.log(probs + epsilon))\n",
        "\n",
        "    return generated_text, semantic_entropy\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"Question: What is the capital of France? Answer:\"\n",
        "generated_text, sem_entropy = custom_generate(input_text, max_new_tokens=50, n_clusters=5)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)\n",
        "print(\"\\nSemantic Entropy: {:.4f}\".format(sem_entropy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wdy_xi-qkCI",
        "outputId": "3de2fa6f-190f-4627-e3f2-0d3a627a0e0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Question: What is the capital of France? Answer: The capital of France is Paris.\n",
            "\n",
            "Semantic Entropy: 1.5596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Entropy with Custom Generation Function and Semantic Entropy Function"
      ],
      "metadata": {
        "id": "xe5ryq8vt68H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def compute_semantic_entropy(last_layer_hidden_states, n_clusters=5):\n",
        "    \"\"\"\n",
        "    Compute semantic entropy from a list of hidden states from the final layer.\n",
        "\n",
        "    Args:\n",
        "        last_layer_hidden_states (list[torch.Tensor]): List of tensors of shape [1, hidden_dim]\n",
        "            representing the hidden state for the last generated token at each generation step.\n",
        "        n_clusters (int): Number of clusters to form for computing entropy.\n",
        "\n",
        "    Returns:\n",
        "        float: The semantic entropy computed from the cluster distribution.\n",
        "    \"\"\"\n",
        "    if not last_layer_hidden_states:\n",
        "        return 0.0\n",
        "\n",
        "    # Stack hidden states into a tensor of shape [num_generated_tokens, hidden_dim]\n",
        "    hidden_tensor = torch.cat(last_layer_hidden_states, dim=0)\n",
        "    num_tokens = hidden_tensor.shape[0]\n",
        "\n",
        "    # Convert to numpy array for clustering\n",
        "    hidden_np = hidden_tensor.cpu().numpy()\n",
        "\n",
        "    # Use at most n_clusters (but not more than number of tokens)\n",
        "    k = min(n_clusters, num_tokens)\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(hidden_np)\n",
        "\n",
        "    # Calculate the probability distribution of tokens across clusters\n",
        "    counts = np.bincount(cluster_labels, minlength=k)\n",
        "    probs = counts / np.sum(counts)\n",
        "\n",
        "    # Compute entropy: -sum(p * log(p))\n",
        "    epsilon = 1e-10  # avoid log(0)\n",
        "    semantic_entropy = -np.sum(probs * np.log(probs + epsilon))\n",
        "\n",
        "    return semantic_entropy\n",
        "\n",
        "def custom_generate(input_text, max_new_tokens=50, n_clusters=5):\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=60).to(device)\n",
        "    input_ids = inputs['input_ids']  # shape: [1, seq_length]\n",
        "\n",
        "    # List to store the hidden states for the last token at each generation step\n",
        "    collected_hidden_states = []  # Each element is a list of per-layer hidden states\n",
        "\n",
        "    # Initialize generated_ids with the input_ids\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Disable caching to recompute full hidden states for the entire sequence\n",
        "            outputs = model(generated_ids, output_hidden_states=True, use_cache=False)\n",
        "            # outputs.hidden_states is a tuple of length (n_layers + 1)\n",
        "            # Extract the hidden states for the last token from each layer\n",
        "            step_hidden = [layer_hidden[0, -1, :].unsqueeze(0) for layer_hidden in outputs.hidden_states]\n",
        "            collected_hidden_states.append(step_hidden)\n",
        "\n",
        "            # Greedy sampling for the next token\n",
        "            next_token_logits = outputs.logits[:, -1, :]  # shape: [1, vocab_size]\n",
        "            next_token_id = next_token_logits.argmax(dim=-1, keepdim=True)  # shape: [1, 1]\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the full generated sequence\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # For semantic entropy, use the hidden states from the final layer for each generated token\n",
        "    # Each element in collected_hidden_states is a list of per-layer representations;\n",
        "    # We take the last element (final layer) from each generation step.\n",
        "    last_layer_hidden_states = [step[-1] for step in collected_hidden_states]  # list of [1, hidden_dim]\n",
        "    if len(last_layer_hidden_states) == 0:\n",
        "        semantic_entropy = 0.0\n",
        "    else:\n",
        "        # Stack into a tensor of shape [num_generated_tokens, hidden_dim]\n",
        "        semantic_entropy = compute_semantic_entropy(last_layer_hidden_states, n_clusters=5)\n",
        "\n",
        "    return generated_text, semantic_entropy\n",
        "\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"Question: What is the capital of France? Answer:\"\n",
        "generated_text, sem_entropy = custom_generate(input_text, max_new_tokens=50, n_clusters=5)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)\n",
        "print(\"\\nSemantic Entropy: {:.4f}\".format(sem_entropy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hnoa5zPs4Le",
        "outputId": "d4383077-b626-4fe0-f515-7f5192724935"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Question: What is the capital of France? Answer: The capital of France is Paris.\n",
            "\n",
            "Semantic Entropy: 1.5596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch: Semantic Entropy with additional forward pass"
      ],
      "metadata": {
        "id": "ahjUrqF_v1py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_answers_with_semantic_entropy(questions, n_clusters=5, max_new_tokens=50,\n",
        "                                             force_min_length=False, min_new_tokens=10):\n",
        "    \"\"\"\n",
        "    For a batch of questions, generate answers and compute semantic entropy for the generated tokens.\n",
        "\n",
        "    Args:\n",
        "        questions (list of str): List of question strings.\n",
        "        n_clusters (int): Number of clusters for KMeans.\n",
        "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "        force_min_length (bool): If True, force a minimum generation length for each sample.\n",
        "        min_new_tokens (int): Number of new tokens to force beyond the prompt if force_min_length is True.\n",
        "\n",
        "    Returns:\n",
        "        answers (list of str): Decoded answers for each question.\n",
        "        semantic_entropies (list of float): Semantic entropy for each answer.\n",
        "    \"\"\"\n",
        "    # Create input texts\n",
        "    input_texts = [f\"Question: {q} Answer:\" for q in questions]\n",
        "\n",
        "    # Batch tokenize with padding\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=60).to(device)\n",
        "    input_ids = inputs['input_ids']  # [batch_size, seq_length]\n",
        "    prompt_lengths = inputs['attention_mask'].sum(dim=1)  # [batch_size]\n",
        "    batch_size = input_ids.shape[0]\n",
        "\n",
        "    # Set generation parameters; disable forced min_length in batch mode if not desired\n",
        "    generation_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"return_dict_in_generate\": True,\n",
        "        \"early_stopping\": True,\n",
        "        \"no_repeat_ngram_size\": 2\n",
        "    }\n",
        "    if force_min_length:\n",
        "        generation_kwargs[\"min_length\"] = input_ids.shape[1] + min_new_tokens\n",
        "\n",
        "    # Generate outputs for the batch\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, **generation_kwargs)\n",
        "\n",
        "    generated_ids = output.sequences  # [batch_size, total_sequence_length]\n",
        "    generated_seq_length = generated_ids.shape[1]\n",
        "    print(f\"Generated sequence length: {generated_seq_length}\")\n",
        "\n",
        "    # Decode full generated texts for each sample\n",
        "    full_texts = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(batch_size)]\n",
        "    answers = [text.split(\"Answer:\")[-1].strip() for text in full_texts]\n",
        "\n",
        "    # Run a forward pass on the full generated sequences to obtain hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(generated_ids, output_hidden_states=True)\n",
        "    last_hidden = outputs.hidden_states[-1]  # shape: [batch_size, total_seq_length, hidden_dim]\n",
        "\n",
        "    semantic_entropies = []\n",
        "    # Compute semantic entropy for each sample\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = int(prompt_lengths[i].item())\n",
        "        generated_hidden = last_hidden[i, prompt_len:, :]  # hidden states for tokens after the prompt\n",
        "        num_tokens = generated_hidden.shape[0]\n",
        "\n",
        "        if num_tokens == 0:\n",
        "            semantic_entropies.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Convert to numpy and cluster\n",
        "        hidden_np = generated_hidden.cpu().numpy()\n",
        "        k = min(n_clusters, num_tokens)\n",
        "        if k == 0:\n",
        "            semantic_entropies.append(0.0)\n",
        "            continue\n",
        "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "        cluster_labels = kmeans.fit_predict(hidden_np)\n",
        "        counts = np.bincount(cluster_labels, minlength=k)\n",
        "        probs = counts / np.sum(counts)\n",
        "        epsilon = 1e-10\n",
        "        semantic_entropy = -np.sum(probs * np.log(probs + epsilon))\n",
        "        semantic_entropies.append(semantic_entropy)\n",
        "\n",
        "    return answers, semantic_entropies\n",
        "\n",
        "# Load GPT-2 model and tokenizer (using your custom model if applicable)\n",
        "model_name = \"./gpt2-qa-best-loss-cml\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example usage with batch size of 2:\n",
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who is the artist behind the Mona Lisa?\"\n",
        "]\n",
        "# For batch mode, we disable forced min_length to let each sample generate naturally.\n",
        "answers, sem_entropies = generate_answers_with_semantic_entropy(questions, n_clusters=5,\n",
        "                                                                  max_new_tokens=50, force_min_length=False)\n",
        "for i, q in enumerate(questions):\n",
        "    print(f\"Q: {q}\\nA: {answers[i]}\\nSemantic Entropy: {sem_entropies[i]:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_vCCtQHwkbT",
        "outputId": "20a1eb45-6397-471b-c57d-564053db4581"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence length: 27\n",
            "Q: What is the capital of France?\n",
            "A: The capital is Paris.\n",
            "Semantic Entropy: 0.9089\n",
            "\n",
            "Q: Who is the artist behind the Mona Lisa?\n",
            "A: The Monma Lisa is Leonardo da Vinci's painting.\n",
            "Semantic Entropy: 1.4791\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGM2JW8mxIVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}