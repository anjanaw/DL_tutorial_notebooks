{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/LoRA_PitVQA_Sentence_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e3HXmQrS6QV",
        "outputId": "2eb06732-5a27-44a5-e35d-a84a6b84bb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/PitVQA/datasets’: No such file or directory\n",
            "[Errno 2] No such file or directory: '/content/PitVQA/datasets'\n",
            "/content\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
            "From (redirected): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78&confirm=t&uuid=f1751d32-321d-48cf-b726-00c624101d75\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.71G/2.71G [00:38<00:00, 69.9MB/s]\n",
            "[Errno 2] No such file or directory: '/content/PitVQA'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#Download Dataset\n",
        "!mkdir /content/PitVQA/datasets\n",
        "%cd /content/PitVQA/datasets\n",
        "!gdown --id 1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\n",
        "!unzip -q EndoVis-18-VQA.zip\n",
        "%cd /content/PitVQA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm==0.9.12 fairscale==0.4.13 scikit-learn==1.3.2 -U evaluate bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOkwHQeZTOZi",
        "outputId": "9c8b15b3-d9ad-4f53-a723-2f513ecc76f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "zn1LPGY3UPnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "class EndoVis18VQA(Dataset):\n",
        "    def __init__(self, seq, folder_head, folder_tail):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),  # input image size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq:\n",
        "            filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' % (len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa_full_path = Path(self.vqas[idx][0])\n",
        "        seq_path = qa_full_path.parents[2]\n",
        "        file_name = self.vqas[idx][0].split('/')[-1]  # / in linux and \\\\ in windows\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(seq_path, 'left_fr', file_name.split('_')[0] + '.png')\n",
        "        raw_image = Image.open(img_loc).convert('RGB')\n",
        "        img = self.transform(raw_image)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "\n",
        "        return img_loc, img, question, answer"
      ],
      "metadata": {
        "id": "-6kiE739TSFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "j9ySe-NcULLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipTextModel\n",
        "from peft import get_peft_model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "class PitVQAGen(nn.Module):\n",
        "    def __init__(self, peft_config=None):\n",
        "        super(PitVQAGen, self).__init__()\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        self.text_encoder = BlipTextModel.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        # copy weights\n",
        "        original_weights = self.text_encoder.embeddings.word_embeddings.weight.data\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        new_embeddings = nn.Embedding(new_vocab_size, embedding_dim)\n",
        "        original_vocab_size = original_weights.shape[0]\n",
        "        new_embeddings.weight.data[:original_vocab_size] = original_weights\n",
        "        self.text_encoder.embeddings.word_embeddings = new_embeddings\n",
        "\n",
        "        # gpt2 decoder\n",
        "        gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt = get_peft_model(gpt, peft_config)\n",
        "        self.gpt.print_trainable_parameters()  # Verify trainable MoRA parameters\n",
        "\n",
        "    def forward(self, image, qa_inputs_ids, qa_att_mask):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n",
        "\n",
        "        # multimodal encoder\n",
        "        text_output = self.text_encoder(input_ids=qa_inputs_ids,\n",
        "                                        attention_mask=qa_att_mask,\n",
        "                                        encoder_hidden_states=image_embeds,\n",
        "                                        encoder_attention_mask=image_atts,\n",
        "                                        return_dict=True)\n",
        "        text_embeds = text_output.last_hidden_state\n",
        "\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=text_embeds,\n",
        "                              encoder_attention_mask=qa_att_mask)\n",
        "        return gpt_output.logits"
      ],
      "metadata": {
        "id": "MmRX4VGiTvU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "yF8S896pVrVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from peft import  TaskType, LoraConfig\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = []\n",
        "\n",
        "    for i, (_, images, questions, answers) in enumerate(train_dataloader, 0):\n",
        "        # prepare prompts\n",
        "        qa_prompt = [f'Question: {q}\\nAnswer: {a}' for q, a in zip(questions, answers)]\n",
        "        qa_prompt_inputs = tokenizer(qa_prompt, truncation=True, padding=\"max_length\", max_length=int(args.seq_length), return_tensors=\"pt\")\n",
        "\n",
        "        # get labels\n",
        "        labels = qa_prompt_inputs['input_ids'].clone()\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # for labels, mask question tokens and padding tokens\n",
        "        for idx, q in enumerate(questions):\n",
        "            q_prompt = f\"Question: {q}\\nAnswer: \"\n",
        "            q_length = len(tokenizer(q_prompt)[\"input_ids\"]) - 1\n",
        "\n",
        "            labels[idx, :q_length] = -100  # mask question\n",
        "            eos_mask = (labels[idx] == tokenizer.eos_token_id)  # get all EOS position\n",
        "            if eos_mask.sum() > 1:  # if more than 1 EOS\n",
        "                first_eos_pos = eos_mask.nonzero()[0].item()  # get first EOS position\n",
        "                labels[idx, (first_eos_pos+1):] = -100  # mask paddings, left one EOS\n",
        "\n",
        "        # get logits and labels\n",
        "        logits = model(\n",
        "                image=images.to(device),\n",
        "                qa_inputs_ids=qa_prompt_inputs['input_ids'].to(device),\n",
        "                qa_att_mask=qa_prompt_inputs['attention_mask'].to(device)\n",
        "        )\n",
        "\n",
        "        # get shifted logits and labels\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        # compute loss\n",
        "        shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "        shift_labels = shift_labels.view(-1)\n",
        "        loss = criterion(shift_logits, shift_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.append(loss.item())\n",
        "    print(\"Training - Epoch: {}/{}, AVG Loss: {:.6f}\".format(epoch, args.epochs, np.array(total_loss).mean()))\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device):\n",
        "    total_loss = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (_, images, questions, answers) in enumerate(val_loader, 0):\n",
        "            # prepare prompts\n",
        "            qa_prompt = [f'Question: {q}\\nAnswer: {a}' for q, a in zip(questions, answers)]\n",
        "            qa_prompt_inputs = tokenizer(qa_prompt, truncation=True, padding=\"max_length\", max_length=int(args.seq_length), return_tensors=\"pt\")\n",
        "\n",
        "            # get labels\n",
        "            labels = qa_prompt_inputs['input_ids'].clone()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # for labels, mask question tokens and padding tokens\n",
        "            answer_starts = []\n",
        "            answer_ends = []\n",
        "            for idx, q in enumerate(questions):\n",
        "                q_prompt = f\"Question: {q}\\nAnswer: \"\n",
        "                q_length = len(tokenizer(q_prompt)[\"input_ids\"]) - 1\n",
        "                answer_starts.append(q_length+1)\n",
        "\n",
        "                labels[idx, :q_length] = -100  # mask question\n",
        "                eos_mask = (labels[idx] == tokenizer.eos_token_id)  # get all EOS position\n",
        "                if eos_mask.sum() > 1:  # if more than 1 EOS\n",
        "                    first_eos_pos = eos_mask.nonzero()[0].item()  # get first EOS position\n",
        "                    labels[idx, (first_eos_pos+1):] = -100  # mask paddings, left one EOS\n",
        "                    answer_ends.append(first_eos_pos)\n",
        "\n",
        "            # get logits and labels\n",
        "            logits = model(\n",
        "                image=images.to(device),\n",
        "                qa_inputs_ids=qa_prompt_inputs['input_ids'].to(device),\n",
        "                qa_att_mask=qa_prompt_inputs['attention_mask'].to(device)\n",
        "            )\n",
        "\n",
        "            # get shifted logits and labels\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "            # compute loss\n",
        "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            loss = criterion(shift_logits, shift_labels)\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "    return np.array(total_loss).mean()\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerGeneration')\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=5,   help='number of epochs to train for')\n",
        "    parser.add_argument('--batch_size',     type=int,   default=32,   help='batch size')\n",
        "    parser.add_argument('--workers',        type=int,   default=8,    help='for data-loading')\n",
        "    parser.add_argument('--random_seed',    type=int,   default=42,   help='random seed')\n",
        "    parser.add_argument('--seq_length',     type=int,   default=64,   help='sequence length for question and answer')\n",
        "    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
        "\n",
        "    parser.add_argument('--dataset',        default='endo',  help='endo / pit')\n",
        "    parser.add_argument('--lr',             type=float, default=0.0000002,  help='0.0000001, 0.00000005')\n",
        "    parser.add_argument('--checkpoint_dir', default='checkpoints/',  help='path to checkpoint')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = get_arg()\n",
        "    seed_everything(args.random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f'Batch size: {args.batch_size}')\n",
        "    print(f'Learning rate: {args.lr}')\n",
        "    print(f'Random seed: {args.random_seed}')\n",
        "    print(f'Sequence length: {args.seq_length}')\n",
        "    print(f'Dropout: {args.dropout}')\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok = True)\n",
        "    start_epoch = 1\n",
        "    epochs_since_improvement = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(f'Dataset: {args.dataset}')\n",
        "    train_dataloader = None\n",
        "    val_dataloader = None\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "    folder_head = '/content/PitVQA/datasets/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    # dataloader\n",
        "    train_dataset = EndoVis18VQA(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
        "                                shuffle=True, num_workers=args.workers)\n",
        "    val_dataset = EndoVis18VQA(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                                shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    # init tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        # target_modules=[\"c_attn\", \"c_proj\"]\n",
        "        target_modules=[\"c_attn\"]\n",
        "    )\n",
        "\n",
        "    model = PitVQAGen(peft_config=lora_config)\n",
        "    model = model.to(device)\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # init optimizer and criterion\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100).to(device)\n",
        "\n",
        "    # train and validation\n",
        "    print('Start training.')\n",
        "    for epoch in range(start_epoch, args.epochs+1):\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer,\n",
        "              epoch=epoch, tokenizer=tokenizer, device=device)\n",
        "        # validation\n",
        "        val_loss = validate(args, val_loader=val_dataloader, model=model, criterion=criterion,\n",
        "                            epoch=epoch, tokenizer=tokenizer, device=device)\n",
        "\n",
        "        if val_loss < best_val_loss:  # save model with better validation loss\n",
        "            epochs_since_improvement = 0\n",
        "            best_val_loss = val_loss\n",
        "            save_dir = f'{args.checkpoint_dir}/best_model.pth'\n",
        "            torch.save(model.state_dict(), save_dir)\n",
        "            model.tokenizer.save_pretrained(args.checkpoint_dir)\n",
        "            print('Best validation loss, model saved.')\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    print('End training.')"
      ],
      "metadata": {
        "id": "MQBpj1WhUqgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference:"
      ],
      "metadata": {
        "id": "uNiUNJvzFRCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipTextModel\n",
        "from peft import get_peft_model\n",
        "from peft import  TaskType, LoraConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_nlp_mettics(references, hypotheses):\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load('meteor')\n",
        "\n",
        "    # compute HF metrics\n",
        "    results_bleu = bleu.compute(predictions=hypotheses, references=references)\n",
        "    results_rouge = rouge.compute(predictions=hypotheses, references=references)\n",
        "    results_meteor = meteor.compute(predictions=hypotheses, references=references)\n",
        "\n",
        "    return results_bleu, results_rouge, results_meteor\n",
        "\n",
        "def batch_greedy_search(images, questions, model, tokenizer, max_length, device):\n",
        "    answers = []\n",
        "    batch_size = len(questions)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare the prompts for the entire batch\n",
        "        prompt_texts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
        "\n",
        "        # Tokenize the prompts with padding to handle varying lengths\n",
        "        prompt_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='longest',\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        # Prepare model inputs\n",
        "        padded_input_ids = torch.zeros((batch_size, max_length), dtype=torch.long, device=device)\n",
        "        padded_attention_mask = torch.zeros((batch_size, max_length), device=device)\n",
        "\n",
        "        orig_length = prompt_inputs['input_ids'].size(1)\n",
        "        padded_input_ids[:, :orig_length] = prompt_inputs['input_ids']\n",
        "        padded_attention_mask[:, :orig_length] = prompt_inputs['attention_mask']\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Initialize tensors to store generated tokens\n",
        "        only_answer_ids = torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
        "\n",
        "        # Track which sequences have finished generating\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        # Record each sample length (number of non-eos tokens)\n",
        "        valid_lengths = padded_attention_mask.sum(dim=1).long()\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            max_valid_lengths = max(valid_lengths).item()\n",
        "            if max_valid_lengths >= max_length:\n",
        "                break  # Stop if any sequence reached max_length\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(image=images, qa_inputs_ids=padded_input_ids[:, :max_valid_lengths], qa_att_mask=padded_attention_mask[:, :max_valid_lengths])\n",
        "\n",
        "            # Get next token probabilities and entropy\n",
        "            last_valid_logits = logits[batch_indices, valid_lengths - 1, :]\n",
        "            probs = F.softmax(last_valid_logits, dim=-1)\n",
        "\n",
        "            # Compute entropy for uncertainty\n",
        "            entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1)  # Add small value to avoid log(0)\n",
        "\n",
        "            # Get next token\n",
        "            next_token_ids = torch.argmax(last_valid_logits, dim=-1)\n",
        "\n",
        "            # Check EOS\n",
        "            is_eos = (next_token_ids == tokenizer.eos_token_id)\n",
        "            finished = finished | is_eos  # Update finished status\n",
        "\n",
        "            padded_input_ids[batch_indices, valid_lengths] = next_token_ids\n",
        "            padded_attention_mask[batch_indices, valid_lengths] = 1\n",
        "            valid_lengths += 1\n",
        "\n",
        "            # Append the selected tokens to the generated_ids\n",
        "            only_answer_ids = torch.cat([only_answer_ids, next_token_ids.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # If all sequences have finished, exit early\n",
        "            if finished.all():\n",
        "                break\n",
        "\n",
        "        # Decode the generated tokens into strings\n",
        "        generated_ids_cpu = only_answer_ids.cpu().tolist()  # Move to CPU and convert to list for processing\n",
        "        for i in range(batch_size):\n",
        "            # Find the first occurrence of eos_token_id to truncate the answer\n",
        "            try:\n",
        "                eos_index = generated_ids_cpu[i].index(tokenizer.eos_token_id)\n",
        "                answer_ids = generated_ids_cpu[i][:eos_index]\n",
        "            except ValueError:\n",
        "                # If eos_token_id is not found, use all generated tokens\n",
        "                answer_ids = generated_ids_cpu[i]\n",
        "\n",
        "            # Decode the token IDs to a string, skipping special tokens\n",
        "            answer = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
        "            answers.append(answer)\n",
        "\n",
        "    return answers\n",
        "\n",
        "def inference(args, val_loader, model, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (_, images, questions, answers) in enumerate(tqdm(val_loader), 0):\n",
        "            images = images.to(device)\n",
        "            generated_answers = batch_greedy_search(\n",
        "                images,\n",
        "                questions,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                max_length=args.seq_length,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            references.extend(answers)\n",
        "            hypotheses.extend(generated_answers)\n",
        "\n",
        "    return references, hypotheses\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    # target_modules=[\"c_attn\", \"c_proj\"]\n",
        "    target_modules=[\"c_attn\"]\n",
        ")\n",
        "\n",
        "model = PitVQAGen(peft_config=lora_config)\n",
        "save_dir = f'{args.checkpoint_dir}/best_model.pth'\n",
        "model.load_state_dict(torch.load(save_dir, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# args.seq_length = 32\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "args.batch_size = 200\n",
        "val_dataset = EndoVis18VQA(val_seq, folder_head, folder_tail)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                            shuffle=False, num_workers=args.workers)\n",
        "\n",
        "\n",
        "references, hypotheses  = inference(args, val_loader=val_dataloader, model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "results_bleu, results_rouge, results_meteor = get_nlp_mettics(references, hypotheses)\n",
        "\n",
        "print(f\"BLEU-1: {results_bleu['precisions'][0]:.6f}, \"\n",
        "      f\"BLEU-2: {results_bleu['precisions'][1]:.6f}, \"\n",
        "      f\"BLEU-3: {results_bleu['precisions'][2]:.6f}, \"\n",
        "      f\"BLEU-4: {results_bleu['precisions'][3]:.6f}, \"\n",
        "      f\"Overall BLEU: {results_bleu['bleu']:.6f}\")\n",
        "\n",
        "\n",
        "print(f\"Rouge1: {results_rouge['rouge1']:.6f}\")\n",
        "print(f\"RougeL: {results_rouge['rougeL']:.6f}\")\n",
        "print(f\"Meteor: {results_meteor['meteor']:.6f}\")"
      ],
      "metadata": {
        "id": "gXZMzL9MAgv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('First 5 Labels:')\n",
        "print(references[:5])\n",
        "\n",
        "print('First 5 Prediction:')\n",
        "print(hypotheses[:5]\n"
      ],
      "metadata": {
        "id": "k0XCriJ4fQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GaEXv-jo5fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}