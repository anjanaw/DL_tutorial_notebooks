{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/PitVQA%2B%2B_inference_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "!gdown 1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
        "\n",
        "# unzip dataset\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO26wWDITwmm",
        "outputId": "654c1092-98d1-44ec-df47-861428d781e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
            "From (redirected): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78&confirm=t&uuid=f64fb8f0-e0cf-4779-8425-0c13a4ab6bdc\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.71G/2.71G [01:06<00:00, 40.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download weights\n",
        "!gdown 1MkKw8sRkG_ffW8sAqbCkdfsY-rECBgLI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVBP30BhUL2V",
        "outputId": "78e07277-19f0-413b-e96b-afdedeb0bf04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MkKw8sRkG_ffW8sAqbCkdfsY-rECBgLI\n",
            "From (redirected): https://drive.google.com/uc?id=1MkKw8sRkG_ffW8sAqbCkdfsY-rECBgLI&confirm=t&uuid=2da09b48-3055-47e6-b40a-798d0a85356a\n",
            "To: /content/vec_mlr_saved_weights.pth.tar\n",
            "100% 3.39G/3.39G [00:57<00:00, 58.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Te3qSOZxmRBbqhT1o4-AMH74a2G9kK5_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRMgPzU4xh5u",
        "outputId": "b9bd514d-ac6a-47cc-bf0b-cc3e647c5e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Te3qSOZxmRBbqhT1o4-AMH74a2G9kK5_\n",
            "From (redirected): https://drive.google.com/uc?id=1Te3qSOZxmRBbqhT1o4-AMH74a2G9kK5_&confirm=t&uuid=90398168-b4ca-4c84-a7c7-b7df6f02e7ce\n",
            "To: /content/model.py\n",
            "\r  0% 0.00/8.65k [00:00<?, ?B/s]\r100% 8.65k/8.65k [00:00<00:00, 33.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install libs\n",
        "!pip install -q timm==0.9.12 fairscale==0.4.13 scikit-learn==1.3.2 evaluate rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roSHkq4IT0BZ",
        "outputId": "71f358eb-0e09-46ac-a32a-fe9a9bec68e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "cmGlSUl1Vn8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from pathlib import Path\n",
        "\n",
        "class EndoVis18VQA(Dataset):\n",
        "    def __init__(self, seq, folder_head, folder_tail):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq:\n",
        "            filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' % (len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa_full_path = Path(self.vqas[idx][0])\n",
        "        seq_path = qa_full_path.parents[2]\n",
        "        file_name = self.vqas[idx][0].split('/')[-1]  # / in linux and \\\\ in windows\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(seq_path, 'left_fr', file_name.split('_')[0] + '.png')\n",
        "        raw_image = Image.open(img_loc).convert('RGB')\n",
        "        img = self.transform(raw_image)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        return img, question, answer"
      ],
      "metadata": {
        "id": "c8I8isToVlND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "FuRN27gFVxAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipTextModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Vector_MoLoRA(nn.Module):\n",
        "    def __init__(self, base_layer, lora_rank, alpha, mora_rank, dropout):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer  # original c_atten layer\n",
        "        self.lora_r = lora_rank\n",
        "        self.scaling = alpha / lora_rank\n",
        "        self.mora_r = mora_rank  # one of mora rank elements in the list\n",
        "        self.in_features = base_layer.weight.shape[0]  # 768\n",
        "        self.out_features = base_layer.weight.shape[1]  # 2304\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.ModuleDict({\n",
        "            'default': nn.Dropout(p=dropout)\n",
        "        })\n",
        "\n",
        "        # MoRA A and B matrices\n",
        "        self.mora_A = nn.ModuleDict({\n",
        "            'default': nn.Conv1d(self.mora_r, self.mora_r, bias=False, kernel_size=1)\n",
        "        })\n",
        "        # zero init for mora_A\n",
        "        nn.init.zeros_(self.mora_A['default'].weight)\n",
        "        self.mora_B = self.mora_A  # not for use\n",
        "\n",
        "        # LoRA C and D matrices\n",
        "        self.lora_C = nn.ModuleDict({\n",
        "            'default': nn.Linear(self.in_features, self.lora_r, bias=False)\n",
        "        })\n",
        "        self.lora_D = nn.ModuleDict({\n",
        "            'default': nn.Linear(self.lora_r, self.out_features, bias=False)\n",
        "        })\n",
        "        # Kaiming init for lora_C\n",
        "        nn.init.kaiming_uniform_(self.lora_C['default'].weight, a=math.sqrt(5))\n",
        "        # zero init for lora_D\n",
        "        nn.init.zeros_(self.lora_D['default'].weight)\n",
        "\n",
        "        # For Embedding layer\n",
        "        self.lora_embedding_A = nn.ParameterDict({})\n",
        "        self.lora_embedding_B = nn.ParameterDict({})\n",
        "\n",
        "    def forward(self, x):  # [32, 32, 768]\n",
        "        # Original output\n",
        "        result = self.base_layer(x)  # [32, 32, 2304]\n",
        "        x = self.dropout['default'](x)  # x is the input for mora\n",
        "\n",
        "        '''Process with LoRA'''\n",
        "        lora_out_x = self.lora_D['default'](\n",
        "            self.lora_C['default'](x)\n",
        "        )  # [32, 32, 2304]\n",
        "\n",
        "        '''Process with MoRA'''\n",
        "        # apply compression before lora_A: RoPE\n",
        "        in_f, out_f = self.in_features, self.out_features\n",
        "        r = self.mora_r\n",
        "        # suppose mora_type = 6\n",
        "        sum_inter = in_f // r\n",
        "        rb1 = in_f // r if in_f % r == 0 else in_f // r + 1\n",
        "\n",
        "        if in_f % r != 0:\n",
        "            pad_size = r - in_f % r\n",
        "            x = torch.cat([x, x[..., :pad_size]], dim=-1)\n",
        "            sum_inter += 1\n",
        "        in_x = x.view(*x.shape[:-1], sum_inter, r)  # [32, 32, 5, 156]\n",
        "\n",
        "        if not hasattr(self, 'cos') and not hasattr(self, 'sin'):\n",
        "            inv_freq = 1.0 / (10000 ** (torch.arange(0, r, 2).float() / r))\n",
        "            t = torch.arange(rb1)\n",
        "            freqs = torch.outer(t, inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            self.cos = emb.cos().unsqueeze(0).to(x.device).to(x.dtype)\n",
        "            self.sin = emb.sin().unsqueeze(0).to(x.device).to(x.dtype)\n",
        "\n",
        "        rh_in_x = torch.cat((-in_x[..., r // 2:], in_x[..., :r // 2]), dim=-1)\n",
        "        in_x = in_x * self.cos + rh_in_x * self.sin  # [32, 32, 3, 256]\n",
        "\n",
        "        # rearrange features\n",
        "        b, c, h, w = in_x.shape  # [16, 32, 3, 256]\n",
        "        in_x = in_x.view(b, c*h, w).permute(0, 2, 1)  # [16, 256, 96]\n",
        "\n",
        "        # apply mora_A\n",
        "        mora_out_x = self.mora_A['default'](in_x)  # [32, 256, 96]\n",
        "        mora_out_x = mora_out_x.view(b, c, h, w)  # [32, 32, 3, 256]\n",
        "\n",
        "        # apply decompression after lora_A\n",
        "        mora_out_x = mora_out_x.view(*x.shape[:-1], -1)[..., :out_f]  # [32, 32, 780]\n",
        "        if mora_out_x.shape[-1] < out_f:\n",
        "            repeat_time = out_f // mora_out_x.shape[-1]\n",
        "            if out_f % mora_out_x.shape[-1] != 0:\n",
        "                repeat_time += 1\n",
        "            mora_out_x = torch.cat([mora_out_x] * repeat_time, dim=-1)[..., :out_f]  # [32, 32, 2304]\n",
        "        return result + mora_out_x + (self.scaling * lora_out_x)\n",
        "\n",
        "class VectorMoLoRAInitializer:\n",
        "    def __init__(self, model, mora_base_rank=8, mora_rank_coefficients=None, lora_rank=None, lora_alpha=None, dropout=0.1):\n",
        "        self.model = model\n",
        "        self.lora_rank = lora_rank\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.mora_base_rank = mora_base_rank\n",
        "        self.dropout = dropout\n",
        "        self.mora_rank_coefficients = mora_rank_coefficients\n",
        "\n",
        "    def calculate_mora_ranks(self):\n",
        "        return [self.mora_base_rank * coeff for coeff in self.mora_rank_coefficients]\n",
        "\n",
        "    def initialize(self):\n",
        "        mora_ranks = self.calculate_mora_ranks()\n",
        "\n",
        "        for param in self.model.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for t_layer_i, blk in enumerate(self.model.transformer.h):\n",
        "            layer_w_qkv = blk.attn.c_attn\n",
        "            current_mora_rank = mora_ranks[t_layer_i]\n",
        "            current_lora_rank = self.lora_rank[t_layer_i]\n",
        "            current_alpha = self.lora_alpha[t_layer_i]\n",
        "            blk.attn.c_attn = Vector_MoLoRA(base_layer=layer_w_qkv, lora_rank=current_lora_rank, alpha=current_alpha,\n",
        "                            mora_rank=current_mora_rank, dropout=self.dropout)\n",
        "\n",
        "        print(\"Vector MoRA params initialized!\")\n",
        "        return self.model\n",
        "\n",
        "class PitVQAGen(nn.Module):\n",
        "    def __init__(self, mora_base_rank=8, mora_rank_coefficients=None, lora_rank=None, lora_alpha=None, dropout=0.1):\n",
        "        super(PitVQAGen, self).__init__()\n",
        "\n",
        "        if mora_rank_coefficients is None or lora_rank is None or lora_alpha is None:\n",
        "            print('Wrong hyperparameters.')\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        self.text_encoder = BlipTextModel.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        original_weights = self.text_encoder.embeddings.word_embeddings.weight.data\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        new_embeddings = nn.Embedding(new_vocab_size, embedding_dim)\n",
        "        original_vocab_size = original_weights.shape[0]\n",
        "        new_embeddings.weight.data[:original_vocab_size] = original_weights\n",
        "        self.text_encoder.embeddings.word_embeddings = new_embeddings\n",
        "\n",
        "        # gpt2 decoder\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt = VectorMoLoRAInitializer(self.gpt, mora_base_rank=mora_base_rank,\n",
        "                                           mora_rank_coefficients=mora_rank_coefficients,\n",
        "                                           lora_rank=lora_rank, lora_alpha=lora_alpha,\n",
        "                                           dropout=dropout).initialize()\n",
        "\n",
        "    def forward(self, image, qa_input_ids, qa_att_mask):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state  # torch.Size([bs, 197, 768])\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)  # torch.Size([bs, 197])\n",
        "\n",
        "        # qa_input_ids = qa_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        # qa_att_mask = qa_inputs['attention_mask']\n",
        "\n",
        "        # multimodal encoder\n",
        "        text_output = self.text_encoder(input_ids=qa_input_ids,\n",
        "                        attention_mask=qa_att_mask,\n",
        "                        encoder_hidden_states=image_embeds,\n",
        "                        encoder_attention_mask=image_atts,\n",
        "                        return_dict=True)\n",
        "        text_embeds = text_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=text_embeds,\n",
        "                    encoder_attention_mask=qa_att_mask)  # torch.Size([bs, 25, 50257])\n",
        "        return gpt_output.logits"
      ],
      "metadata": {
        "id": "Rh70buhXVyP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "w47_0OKuWNCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def batch_greedy_search(images, questions, model, tokenizer, max_length, device):\n",
        "    answers = []\n",
        "    batch_size = len(questions)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # prepare the prompts for the entire batch\n",
        "        prompt_texts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
        "\n",
        "        # Tokenize the prompts with padding to handle varying lengths\n",
        "        prompt_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='longest',\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        # prepare model inputs\n",
        "        padded_input_ids = torch.zeros((batch_size, max_length), dtype=torch.long, device=device)\n",
        "        padded_attention_mask = torch.zeros((batch_size, max_length), device=device)\n",
        "\n",
        "        orig_length = prompt_inputs['input_ids'].size(1)\n",
        "        padded_input_ids[:, :orig_length] = prompt_inputs['input_ids']\n",
        "        padded_attention_mask[:, :orig_length] = prompt_inputs['attention_mask']\n",
        "\n",
        "        generated_inputs = {\n",
        "            'input_ids': padded_input_ids,\n",
        "            'attention_mask': padded_attention_mask\n",
        "        }\n",
        "        images = images.to(device)\n",
        "\n",
        "        # initialize tensors to store generated tokens\n",
        "        only_answer_ids = torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
        "        # track which sequences have finished generating\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "        # record each sample length (number of non-eos tokens)\n",
        "        valid_lengths = padded_attention_mask.sum(dim=1).long()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # forward pass through the model\n",
        "            # logits = model(image=images, qa_inputs=generated_inputs)\n",
        "            logits = model(image=images, qa_input_ids=padded_input_ids, qa_att_mask=padded_attention_mask)\n",
        "\n",
        "            # get next token\n",
        "            batch_indices = torch.arange(batch_size, device=device)\n",
        "            last_valid_logits = logits[batch_indices, valid_lengths - 1, :]\n",
        "            next_token_ids = torch.argmax(last_valid_logits, dim=-1)\n",
        "\n",
        "            # check eos\n",
        "            is_eos = (next_token_ids == tokenizer.eos_token_id)\n",
        "            finished = finished | is_eos\n",
        "\n",
        "            generated_inputs['input_ids'][batch_indices, valid_lengths] = next_token_ids\n",
        "            generated_inputs['attention_mask'][batch_indices, valid_lengths] = 1\n",
        "            valid_lengths += 1\n",
        "\n",
        "            # mask = (valid_lengths < max_length)  # if max_length is not big enough\n",
        "            # generated_inputs['input_ids'][batch_indices[mask], valid_lengths[mask]] = next_token_ids[mask]\n",
        "            # generated_inputs['attention_mask'][batch_indices[mask], valid_lengths[mask]] = 1\n",
        "            # valid_lengths[mask] += 1\n",
        "\n",
        "            # append the selected tokens to the generated_ids\n",
        "            only_answer_ids = torch.cat([only_answer_ids, next_token_ids.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # if all sequences have finished, exit early\n",
        "            if finished.all():\n",
        "                break\n",
        "\n",
        "        # decode the generated tokens into strings\n",
        "        generated_ids_cpu = only_answer_ids.cpu().tolist()  # Move to CPU and convert to list for processing\n",
        "        for i in range(batch_size):\n",
        "            # Find the first occurrence of eos_token_id to truncate the answer\n",
        "            try:\n",
        "                eos_index = generated_ids_cpu[i].index(tokenizer.eos_token_id)\n",
        "                answer_ids = generated_ids_cpu[i][:eos_index]\n",
        "            except ValueError:\n",
        "                # If eos_token_id is not found, use all generated tokens\n",
        "                answer_ids = generated_ids_cpu[i]\n",
        "\n",
        "            # decode the token IDs to a string, skipping special tokens\n",
        "            answer = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
        "            answers.append(answer)\n",
        "    return answers\n",
        "\n",
        "def validate(args, val_loader, model, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load('meteor')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, questions, answers) in enumerate(tqdm(val_loader), 0):\n",
        "            images = images.to(device)\n",
        "            generated_answers = batch_greedy_search(\n",
        "                images,\n",
        "                questions,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                max_length=args.seq_length,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for ref, hyp in zip(answers, generated_answers):\n",
        "                references.append([ref.split()])\n",
        "                hypotheses.append(hyp.split())\n",
        "\n",
        "        ref_sentence = [' '.join(ref[0]) for ref in references]\n",
        "        hyp_sentence = [' '.join(hyp) for hyp in hypotheses]\n",
        "\n",
        "        # compute HF metrics\n",
        "        results_bleu = bleu.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        results_rouge = rouge.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        results_meteor = meteor.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        print(\"HuggingFace Metrics Results:\")\n",
        "        print(f\"BLEU-4: {results_bleu['bleu']:.6f}\")\n",
        "        print(f\"Rouge1: {results_rouge['rouge1']:.6f}\")\n",
        "        print(f\"RougeL: {results_rouge['rougeL']:.6f}\")\n",
        "        print(f\"Meteor: {results_meteor['meteor']:.6f}\")\n",
        "\n",
        "        # compute BLEU 1-4\n",
        "        metrics = {\n",
        "            \"Bleu_1\": corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00)),\n",
        "            \"Bleu_2\": corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00)),\n",
        "            \"Bleu_3\": corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00)),\n",
        "            \"Bleu_4\": corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "        print(\"\\nNLTK BLEU Scores:\")\n",
        "        for metric_name, score in metrics.items():\n",
        "            print(f\"{metric_name}: {score:.6f}\")\n",
        "    return metrics\n",
        "\n",
        "class HyperPara:\n",
        "    def __init__(self):\n",
        "        self.epochs = 60\n",
        "        self.seq_length = 64\n",
        "        self.workers = 4\n",
        "        self.batch_size = 256\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parameters\n",
        "    mora_base_rank = 8\n",
        "    mora_coeff = [26, 26, 24, 24, 22, 22, 20, 20, 18, 18, 16, 16]\n",
        "    lora_rank = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    lora_alpha = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    dropout = 0.1\n",
        "\n",
        "    random_seed = 42\n",
        "    seed_everything(random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load weights\n",
        "    model_path = '/content/vec_mlr_saved_weights.pth.tar'\n",
        "    model = PitVQAGen(mora_base_rank=mora_base_rank, mora_rank_coefficients=mora_coeff,\n",
        "              lora_rank=lora_rank, lora_alpha=lora_alpha, dropout=dropout)\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    saved_model = checkpoint['model']\n",
        "    saved_state_dict = saved_model.state_dict()\n",
        "    model.load_state_dict(saved_state_dict)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # prepare data\n",
        "    args = HyperPara()\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    val_dataset = EndoVis18VQA(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                                shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    st = time.time()\n",
        "    val_loss = validate(args, val_loader=val_dataloader, model=model, tokenizer=tokenizer, device=device)\n",
        "    en = time.time()\n",
        "    print('total time:', en -st)"
      ],
      "metadata": {
        "id": "TATwW29A3qW4",
        "outputId": "0053fd3e-9da4-4d03-fa5f-4fbfa926b649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-vqa-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector MoRA params initialized!\n",
            "Total files: 447 | Total question: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "100%|██████████| 13/13 [15:19<00:00, 70.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace Metrics Results:\n",
            "BLEU-4: 0.793235\n",
            "Rouge1: 0.894141\n",
            "RougeL: 0.894226\n",
            "Meteor: 0.862613\n",
            "\n",
            "NLTK BLEU Scores:\n",
            "Bleu_1: 0.822409\n",
            "Bleu_2: 0.814723\n",
            "Bleu_3: 0.806443\n",
            "Bleu_4: 0.793235\n",
            "total time: 931.6238055229187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Greedy Search V2:"
      ],
      "metadata": {
        "id": "4SgF4mMh7oSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def batch_greedy_search(images, questions, model, tokenizer, max_length, device):\n",
        "    answers = []\n",
        "    batch_size = len(questions)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # prepare the prompts for the entire batch\n",
        "        prompt_texts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
        "\n",
        "        # Tokenize the prompts with padding to handle varying lengths\n",
        "        prompt_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='longest',\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        images = images.to(device)\n",
        "        input_ids = prompt_inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = prompt_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Add padding token to handle dynamic length expansion\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        input_ids = torch.cat([input_ids, torch.full((input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
        "        attention_mask = torch.cat([attention_mask, torch.zeros((attention_mask.size(0), 1), dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "        # Track the length of each input prompt\n",
        "        input_lengths = attention_mask.sum(dim=1)  # Number of non-padding tokens per input\n",
        "        initial_input_lengths = input_lengths.clone()\n",
        "\n",
        "        batch_size = len(questions)\n",
        "\n",
        "        for idx in range(max_length):\n",
        "            # forward pass through the model\n",
        "            logits = model(image=images, qa_input_ids=input_ids, qa_att_mask=attention_mask)\n",
        "\n",
        "            # Select the token using input_lengths to avoid padding issues\n",
        "            next_token_logits = logits[torch.arange(batch_size), input_lengths - 1, :]\n",
        "            next_token_ids = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
        "\n",
        "            # Stop generating if EOS token is predicted for all sequences\n",
        "            eos_reached = next_token_ids == tokenizer.eos_token_id\n",
        "            if eos_reached.all():\n",
        "                break\n",
        "\n",
        "            # Append predicted tokens and update attention mask\n",
        "            input_ids.scatter_(1, input_lengths.unsqueeze(1), next_token_ids)\n",
        "            attention_mask.scatter_(1, input_lengths.unsqueeze(1), torch.ones_like(next_token_ids))\n",
        "            input_lengths += 1\n",
        "\n",
        "            # Expand input_ids and attention_mask for further predictions\n",
        "            input_ids = torch.cat([input_ids, torch.full((batch_size, 1), pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.zeros((batch_size, 1), dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "        # Extract only the generated part by slicing from input length onwards\n",
        "        answers = []\n",
        "        for i in range(batch_size):\n",
        "            generated_answer_ids = input_ids[i, initial_input_lengths[i]:]\n",
        "            eos_idx = (generated_answer_ids == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
        "            if eos_idx.numel() > 0:\n",
        "                generated_answer_ids = generated_answer_ids[:eos_idx[0]]\n",
        "\n",
        "            answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
        "            answers.append(answer)\n",
        "\n",
        "    return answers\n",
        "\n",
        "def validate(args, val_loader, model, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    answers_logits_all = torch.empty(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, questions, answers) in enumerate(tqdm(val_loader), 0):\n",
        "            images = images.to(device)\n",
        "            generated_answers = batch_greedy_search(\n",
        "                images,\n",
        "                questions,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                max_length=args.seq_length,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            references.extend(answers)\n",
        "            hypotheses.extend(generated_answers)\n",
        "\n",
        "    return references, hypotheses\n",
        "\n",
        "def get_nlp_mettics(references, hypotheses):\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load('meteor')\n",
        "\n",
        "    # compute HF metrics\n",
        "    results_bleu = bleu.compute(predictions=hypotheses, references=references)\n",
        "    results_rouge = rouge.compute(predictions=hypotheses, references=references)\n",
        "    results_meteor = meteor.compute(predictions=hypotheses, references=references)\n",
        "\n",
        "    print(\"HuggingFace Metrics Results:\")\n",
        "    print(f\"BLEU-4: {results_bleu['bleu']:.6f}\")\n",
        "    print(f\"Rouge1: {results_rouge['rouge1']:.6f}\")\n",
        "    print(f\"RougeL: {results_rouge['rougeL']:.6f}\")\n",
        "    print(f\"Meteor: {results_meteor['meteor']:.6f}\")\n",
        "\n",
        "\n",
        "class HyperPara:\n",
        "    def __init__(self):\n",
        "        self.epochs = 60\n",
        "        self.seq_length = 64\n",
        "        self.workers = 4\n",
        "        self.batch_size = 256\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parameters\n",
        "    mora_base_rank = 8\n",
        "    mora_coeff = [26, 26, 24, 24, 22, 22, 20, 20, 18, 18, 16, 16]\n",
        "    lora_rank = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    lora_alpha = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    dropout = 0.1\n",
        "\n",
        "    random_seed = 42\n",
        "    seed_everything(random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load weights\n",
        "    model_path = '/content/vec_mlr_saved_weights.pth.tar'\n",
        "    model = PitVQAGen(mora_base_rank=mora_base_rank, mora_rank_coefficients=mora_coeff,\n",
        "              lora_rank=lora_rank, lora_alpha=lora_alpha, dropout=dropout)\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    saved_model = checkpoint['model']\n",
        "    saved_state_dict = saved_model.state_dict()\n",
        "    model.load_state_dict(saved_state_dict)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # prepare data\n",
        "    args = HyperPara()\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    val_dataset = EndoVis18VQA(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                                shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                                    shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    st = time.time()\n",
        "    references, hypotheses = validate(args, val_loader=val_dataloader, model=model, tokenizer=tokenizer, device=device)\n",
        "    en = time.time()\n",
        "    print('total time:', en -st)\n",
        "\n",
        "    get_nlp_mettics(references, hypotheses)"
      ],
      "metadata": {
        "id": "syj_XZY9By1i",
        "outputId": "df014d58-8dc2-4af6-ddff-597343e6d688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-vqa-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector MoRA params initialized!\n",
            "Total files: 447 | Total question: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/13 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear Memory"
      ],
      "metadata": {
        "id": "D_n9xBjJFFTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "uyBCYaPcmJmC",
        "outputId": "5695f740-8bdd-43d5-c69c-10ba15f71b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated memory: 0.01 GB\n",
            "GPU reserved memory: 0.02 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Beam Search"
      ],
      "metadata": {
        "id": "0Y6MAWC3JSP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def batch_beam_search(images, questions, model, tokenizer, max_length, device):\n",
        "    answers = []\n",
        "    batch_size = len(questions)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # prepare the prompts for the entire batch\n",
        "        prompt_texts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
        "\n",
        "        # Tokenize the prompts with padding to handle varying lengths\n",
        "        prompt_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='longest',\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        # prepare model inputs\n",
        "        padded_input_ids = torch.zeros((batch_size, max_length), dtype=torch.long, device=device)\n",
        "        padded_attention_mask = torch.zeros((batch_size, max_length), device=device)\n",
        "\n",
        "        orig_length = prompt_inputs['input_ids'].size(1)\n",
        "        padded_input_ids[:, :orig_length] = prompt_inputs['input_ids']\n",
        "        padded_attention_mask[:, :orig_length] = prompt_inputs['attention_mask']\n",
        "\n",
        "        generated_inputs = {\n",
        "            'input_ids': padded_input_ids,\n",
        "            'attention_mask': padded_attention_mask\n",
        "        }\n",
        "        images = images.to(device)\n",
        "\n",
        "        # initialize tensors to store generated tokens\n",
        "        only_answer_ids = torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
        "        # track which sequences have finished generating\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "        # record each sample length (number of non-eos tokens)\n",
        "        valid_lengths = padded_attention_mask.sum(dim=1).long()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # forward pass through the model\n",
        "            logits = model(image=images, qa_inputs=generated_inputs)\n",
        "\n",
        "            # get top-k tokens (beam search with k=5)\n",
        "            batch_indices = torch.arange(batch_size, device=device)\n",
        "            last_valid_logits = logits[batch_indices, valid_lengths - 1, :]\n",
        "            next_token_ids = torch.topk(last_valid_logits, 5, dim=-1).indices[:, 0]\n",
        "\n",
        "            # check eos\n",
        "            is_eos = (next_token_ids == tokenizer.eos_token_id)\n",
        "            finished = finished | is_eos\n",
        "\n",
        "            generated_inputs['input_ids'][batch_indices, valid_lengths] = next_token_ids\n",
        "            generated_inputs['attention_mask'][batch_indices, valid_lengths] = 1\n",
        "            valid_lengths += 1\n",
        "\n",
        "            only_answer_ids = torch.cat([only_answer_ids, next_token_ids.unsqueeze(1)], dim=1)\n",
        "\n",
        "            if finished.all():\n",
        "                break\n",
        "\n",
        "        # decode the generated tokens into strings\n",
        "        generated_ids_cpu = only_answer_ids.cpu().tolist()\n",
        "        for i in range(batch_size):\n",
        "            try:\n",
        "                eos_index = generated_ids_cpu[i].index(tokenizer.eos_token_id)\n",
        "                answer_ids = generated_ids_cpu[i][:eos_index]\n",
        "            except ValueError:\n",
        "                answer_ids = generated_ids_cpu[i]\n",
        "\n",
        "            answer = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
        "            answers.append(answer)\n",
        "    return answers\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load('meteor')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, questions, answers) in enumerate(tqdm(val_loader), 0):\n",
        "            images = images.to(device)\n",
        "            generated_answers = batch_beam_search(\n",
        "                images,\n",
        "                questions,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                max_length=args.seq_length,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for ref, hyp in zip(answers, generated_answers):\n",
        "                references.append([ref.split()])\n",
        "                hypotheses.append(hyp.split())\n",
        "\n",
        "        ref_sentence = [' '.join(ref[0]) for ref in references]\n",
        "        hyp_sentence = [' '.join(hyp) for hyp in hypotheses]\n",
        "\n",
        "        # compute HF metrics\n",
        "        results_bleu = bleu.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        results_rouge = rouge.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        results_meteor = meteor.compute(predictions=hyp_sentence, references=ref_sentence)\n",
        "        print(\"HuggingFace Metrics Results:\")\n",
        "        print(f\"BLEU-4: {results_bleu['bleu']:.6f}\")\n",
        "        print(f\"Rouge1: {results_rouge['rouge1']:.6f}\")\n",
        "        print(f\"RougeL: {results_rouge['rougeL']:.6f}\")\n",
        "        print(f\"Meteor: {results_meteor['meteor']:.6f}\")\n",
        "\n",
        "        # compute BLEU 1-4\n",
        "        metrics = {\n",
        "            \"Bleu_1\": corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00)),\n",
        "            \"Bleu_2\": corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00)),\n",
        "            \"Bleu_3\": corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00)),\n",
        "            \"Bleu_4\": corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "        print(\"\\nNLTK BLEU Scores:\")\n",
        "        for metric_name, score in metrics.items():\n",
        "            print(f\"{metric_name}: {score:.6f}\")\n",
        "    return metrics\n",
        "\n",
        "class HyperPara:\n",
        "    def __init__(self):\n",
        "        self.epochs = 60\n",
        "        self.seq_length = 64\n",
        "        self.workers = 4\n",
        "        self.batch_size = 256\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parameters\n",
        "    mora_base_rank = 8\n",
        "    mora_coeff = [26, 26, 24, 24, 22, 22, 20, 20, 18, 18, 16, 16]\n",
        "    lora_rank = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    lora_alpha = [18, 18, 16, 16, 14, 14, 12, 12, 10, 10, 8, 8]\n",
        "    dropout = 0.1\n",
        "\n",
        "    random_seed = 42\n",
        "    seed_everything(random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load weights\n",
        "    model_path = '/content/vec_mlr_saved_weights.pth.tar'\n",
        "    model = PitVQAGen(mora_base_rank=mora_base_rank, mora_rank_coefficients=mora_coeff,\n",
        "              lora_rank=lora_rank, lora_alpha=lora_alpha, dropout=dropout)\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    saved_model = checkpoint['model']\n",
        "    saved_state_dict = saved_model.state_dict()\n",
        "    model.load_state_dict(saved_state_dict)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # prepare data\n",
        "    args = HyperPara()\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    val_dataset = EndoVis18VQA(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                                shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    st = time.time()\n",
        "    val_loss = validate(args, val_loader=val_dataloader, model=model, tokenizer=tokenizer, device=device)\n",
        "    en = time.time()\n",
        "    print('total time:', en -st)"
      ],
      "metadata": {
        "id": "6Wsv47QW50C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c98b681-c4e8-4967-84d3-f4d35f5bbf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-vqa-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector MoRA params initialized!\n",
            "Total files: 447 | Total question: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "100%|██████████| 13/13 [14:10<00:00, 65.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace Metrics Results:\n",
            "BLEU-4: 0.793235\n",
            "Rouge1: 0.894113\n",
            "RougeL: 0.894203\n",
            "Meteor: 0.862613\n",
            "\n",
            "NLTK BLEU Scores:\n",
            "Bleu_1: 0.822409\n",
            "Bleu_2: 0.814723\n",
            "Bleu_3: 0.806443\n",
            "Bleu_4: 0.793235\n",
            "total time: 857.453047990799\n"
          ]
        }
      ]
    }
  ]
}