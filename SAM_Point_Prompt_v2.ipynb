{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN2LofVJiLxVhUpXHCuJsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAM_Point_Prompt_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4YqCMa_xSKK"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip -q install opencv-python-headless matplotlib\n",
        "!pip -q install bitsandbytes transformers accelerate peft\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Kitti Sementation Dataset"
      ],
      "metadata": {
        "id": "V4Hs2FwYxdcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1EB9JSbcQIqjwI5wc8idMWEjmL3CRhh2D\n",
        "!unzip -q kitti_autonomous_driving_seg.zip"
      ],
      "metadata": {
        "id": "oKKzIXMmxdzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset and Dataloader:"
      ],
      "metadata": {
        "id": "YFsEgtwTxhey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Class names for visualization\n",
        "class_names = [\n",
        "    \"unlabeled\", \"ego vehicle\", \"rectification border\", \"out of roi\", \"static\", \"dynamic\", \"ground\", \"road\",\n",
        "    \"sidewalk\", \"parking\", \"rail track\", \"building\", \"wall\", \"fence\", \"guard rail\", \"bridge\", \"tunnel\", \"pole\",\n",
        "    \"polegroup\", \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\", \"truck\",\n",
        "    \"bus\", \"caravan\", \"trailer\", \"train\", \"motorcycle\", \"bicycle\"\n",
        "]\n",
        "\n",
        "class KITTISegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.images_dir = os.path.join(root_dir, 'images')\n",
        "        self.masks_dir = os.path.join(root_dir, 'masks')\n",
        "        self.image_files = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.png')])\n",
        "        self.mask_files = sorted([f for f in os.listdir(self.masks_dir) if f.endswith('.png')])\n",
        "\n",
        "        # Define resize transformation for both image and mask\n",
        "        self.target_size=(256, 512)\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize(self.target_size),  # Resize image\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_files[idx])\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')  # Load as grayscale\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "        mask = mask.resize((self.target_size[1], self.target_size[0]), resample=Image.NEAREST)\n",
        "\n",
        "        # Convert mask to tensor without scaling\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        return image, mask\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset_train = KITTISegmentationDataset(root_dir='kitti_autonomous_driving_seg/train')\n",
        "dataset_test = KITTISegmentationDataset(root_dir='kitti_autonomous_driving_seg/test')\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=12, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=12, shuffle=True)\n",
        "print('training sample:', len(dataset_train), 'testing sample:', len(dataset_test))\n",
        "print('1st sample shape:',dataset_train[0][0].shape, 'classes inside masks:', dataset_train[0][1].unique())"
      ],
      "metadata": {
        "id": "cF_vQ757xh-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validating point prompt with random point of the classes: sky or road"
      ],
      "metadata": {
        "id": "6iV5-jQVxmlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load SAM model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"  # Download from SAM GitHub release page\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "sam.to(device)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Class names for visualization\n",
        "class_names = [\n",
        "    \"unlabeled\", \"ego vehicle\", \"rectification border\", \"out of roi\", \"static\", \"dynamic\", \"ground\", \"road\",\n",
        "    \"sidewalk\", \"parking\", \"rail track\", \"building\", \"wall\", \"fence\", \"guard rail\", \"bridge\", \"tunnel\", \"pole\",\n",
        "    \"polegroup\", \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\", \"truck\",\n",
        "    \"bus\", \"caravan\", \"trailer\", \"train\", \"motorcycle\", \"bicycle\"\n",
        "]\n",
        "\n",
        "# Function to extract a random point prompt from the target class in the mask\n",
        "def get_random_point_from_class(mask, class_name):\n",
        "    class_index = class_names.index(class_name)\n",
        "    mask_np = mask.cpu().numpy()\n",
        "\n",
        "    # Find coordinates of the target class\n",
        "    target_points = np.argwhere(mask_np == class_index)\n",
        "    if len(target_points) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # Select a random point from the target class region\n",
        "    random_idx = np.random.choice(len(target_points))\n",
        "    y, x = target_points[random_idx]\n",
        "\n",
        "    point = np.array([[x, y]])  # Format for SAM: [[x, y]]\n",
        "    label = np.array([1])  # 1 = Foreground\n",
        "    return point, label\n",
        "\n",
        "# Load dataset and dataloader (for validation)\n",
        "dataset_test = KITTISegmentationDataset(root_dir='kitti_autonomous_driving_seg/test')\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
        "\n",
        "# Get a batch of data from the validation dataloader\n",
        "images, masks = next(iter(dataloader_test))\n",
        "\n",
        "# Extract image and mask\n",
        "image = images[0].permute(1, 2, 0).cpu().numpy()  # Convert to HWC format for SAM\n",
        "mask = masks[0]\n",
        "\n",
        "# ðŸ”¥ Specify the target class ('sky' or 'road')\n",
        "target_class = \"road\"  # Change to \"sky\" if needed\n",
        "point, label = get_random_point_from_class(mask, target_class)\n",
        "\n",
        "if point is not None:\n",
        "    # Run SAM prediction\n",
        "    predictor.set_image(image)\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=point,\n",
        "        point_labels=label,\n",
        "        multimask_output=False  # Get a single best mask\n",
        "    )\n",
        "\n",
        "    # ---- Visualization ----\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Input Image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Input Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Ground Truth Mask with Random Point Overlay\n",
        "    axes[1].imshow(mask.cpu().numpy(), cmap='gray')\n",
        "    axes[1].scatter(\n",
        "        point[0][0], point[0][1],  # x, y coordinates\n",
        "        color='red', s=100, marker='o', edgecolors='white'\n",
        "    )\n",
        "    axes[1].set_title(f'Ground Truth Mask with \"{target_class}\" Random Point')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Predicted Mask from SAM (WITHOUT Point Overlay)\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(masks[0], alpha=0.5, cmap='viridis')\n",
        "    axes[2].set_title(f'Predicted Mask for \"{target_class}\"')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"No valid points found for class '{target_class}'.\")\n"
      ],
      "metadata": {
        "id": "YNcIpnFwxnML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAM LoRA finetuning for sky and road only with point prompt"
      ],
      "metadata": {
        "id": "78sYVbpcx2EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import random\n",
        "\n",
        "# Class names for visualization\n",
        "class_names = [\n",
        "    \"unlabeled\", \"ego vehicle\", \"rectification border\", \"out of roi\", \"static\", \"dynamic\", \"ground\", \"road\",\n",
        "    \"sidewalk\", \"parking\", \"rail track\", \"building\", \"wall\", \"fence\", \"guard rail\", \"bridge\", \"tunnel\", \"pole\",\n",
        "    \"polegroup\", \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\", \"truck\",\n",
        "    \"bus\", \"caravan\", \"trailer\", \"train\", \"motorcycle\", \"bicycle\"\n",
        "]\n",
        "\n",
        "# Class IDs for sky and road\n",
        "sky_id = class_names.index(\"sky\")\n",
        "road_id = class_names.index(\"road\")\n",
        "\n",
        "class KITTISegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.images_dir = os.path.join(root_dir, 'images')\n",
        "        self.masks_dir = os.path.join(root_dir, 'masks')\n",
        "        self.image_files = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.png')])\n",
        "        self.mask_files = sorted([f for f in os.listdir(self.masks_dir) if f.endswith('.png')])\n",
        "\n",
        "        # Define resize transformation for both image and mask\n",
        "        self.target_size = (256, 512)\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize(self.target_size),  # Resize image\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_files[idx])\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')  # Load as grayscale\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "        mask = mask.resize((self.target_size[1], self.target_size[0]), resample=Image.NEAREST)\n",
        "\n",
        "        # Convert mask to numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Filter mask to only include sky and road classes\n",
        "        mask_filtered = np.zeros_like(mask)\n",
        "        mask_filtered[mask == sky_id] = 1  # Sky class\n",
        "        mask_filtered[mask == road_id] = 2  # Road class\n",
        "\n",
        "        # Convert mask to tensor\n",
        "        mask_filtered = torch.from_numpy(mask_filtered).long()\n",
        "\n",
        "        # Generate random point prompt for sky or road class\n",
        "        target_class = random.choice([1, 2])  # 1: Sky, 2: Road\n",
        "        target_mask = (mask_filtered == target_class).numpy()\n",
        "\n",
        "        # Get random point within the target class region\n",
        "        if np.any(target_mask):\n",
        "            y, x = np.where(target_mask)\n",
        "            random_idx = np.random.choice(len(y))\n",
        "            point = np.array([[x[random_idx], y[random_idx]]])  # Point in (x, y) format\n",
        "            label = np.array([1])  # Label 1 for foreground\n",
        "        else:\n",
        "            point = np.array([[-1, -1]])  # Invalid point\n",
        "            label = np.array([-1])  # Invalid label\n",
        "\n",
        "        return image, mask_filtered, point, label, target_class\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset_train = KITTISegmentationDataset(root_dir='kitti_autonomous_driving_seg/train')\n",
        "dataset_test = KITTISegmentationDataset(root_dir='kitti_autonomous_driving_seg/test')\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=12, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=12, shuffle=True)\n",
        "\n",
        "# Load SAM model\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(sam.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    sam.train()\n",
        "    for images, masks, points, labels, target_classes in dataloader_train:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        points, labels = points.to(device), labels.to(device)\n",
        "\n",
        "        # Initialize a list to store predicted masks\n",
        "        masks_pred_list = []\n",
        "\n",
        "        # Process each image in the batch individually\n",
        "        for i in range(images.shape[0]):\n",
        "            # Get the i-th image and corresponding point/label\n",
        "            image = images[i].cpu().numpy().transpose(1, 2, 0)  # Convert to [H, W, C]\n",
        "            point = points[i].cpu().numpy()\n",
        "            label = labels[i].cpu().numpy()\n",
        "\n",
        "            # Set the image in the predictor\n",
        "            predictor.set_image(image)\n",
        "\n",
        "            # Predict the mask using the point prompt\n",
        "            masks_pred, _, _ = predictor.predict(\n",
        "                point_coords=point,\n",
        "                point_labels=label,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "            # Convert the predicted mask to logits\n",
        "            masks_pred = masks_pred.astype(np.float32)  # Convert boolean to float\n",
        "            masks_pred = torch.from_numpy(masks_pred).to(device)\n",
        "\n",
        "            # Append the predicted mask to the list\n",
        "            masks_pred_list.append(masks_pred)\n",
        "\n",
        "        # Stack the predicted masks into a batch\n",
        "        masks_pred = torch.stack(masks_pred_list, dim=0)\n",
        "\n",
        "        # Ensure the target masks are in the correct format\n",
        "        masks = masks.float()  # Convert target masks to float\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(masks_pred, masks)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "O3a7mR8ux2sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}