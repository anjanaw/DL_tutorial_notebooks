{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHIr7Wa1xwJlLX0RGWSX0D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Truncated_Galore_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vRwt4hZcy3v",
        "outputId": "0805daf7-3286-45f5-c4df-1988d3c8b808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GaLore'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 146 (delta 57), reused 45 (delta 42), pack-reused 56 (from 2)\u001b[K\n",
            "Receiving objects: 100% (146/146), 442.55 KiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "/content/GaLore\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jiaweizzhao/GaLore.git\n",
        "%cd GaLore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tensorly transformers peft bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YcQ8QsOjxMN",
        "outputId": "04fe8cea-b9d1-473d-e21b-a19afc2936fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tensorly transformers==4.48.0 peft bitsandbytes==0.45.3 accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuIPWfi_dHqQ",
        "outputId": "8a6d5d14-0af6-4ef8-c474-7254bf4a629f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers==4.46.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnYoEJ7vjRZY",
        "outputId": "bafb2a7c-216e-4f2d-e19b-77722b5f6547"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
            "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRPjaGcLg_Dn",
        "outputId": "39126472-7643-491c-a371-4ec00df3c3b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.46.0\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GaLore\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-afzk5ec6ir",
        "outputId": "fab96dcb-ead7-4c65-8723-0e8e9ac19ca7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GaLore\n",
            "CITATION.cff\t      galore_torch  peft_pretraining  run_glue.py  torchrun_main.py\n",
            "configs\t\t      imgs\t    README.md\t      scripts\n",
            "exp_requirements.txt  LICENSE\t    requirements.txt  setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import bitsandbytes\""
      ],
      "metadata": {
        "id": "Q8V_pux7huRn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare model"
      ],
      "metadata": {
        "id": "htSzWuOYmK5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 (better than standard FP4)\n",
        "    bnb_4bit_use_double_quant=True,  # Uses secondary quantization for better precision\n",
        "    bnb_4bit_compute_dtype=torch.float16  # Keeps computation in FP16 for stability\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "yMqHzkeHeYmo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply galore"
      ],
      "metadata": {
        "id": "yI-aLyvbmNii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor\n",
        "\n",
        "# make parameters with \"rank\" to a single group, if param_name has \"mlp\" or \"attn\"\n",
        "galore_params = []\n",
        "target_modules_list = [\"attn\", \"mlp\"]\n",
        "for module_name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "\n",
        "    if not any(target_key in module_name for target_key in target_modules_list):\n",
        "        continue\n",
        "\n",
        "    # print('enable GaLore for weights in module: ', module_name)\n",
        "    galore_params.append(module.weight)\n",
        "id_galore_params = [id(p) for p in galore_params]\n",
        "# make parameters without \"rank\" to another group\n",
        "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
        "# then call galore_adamw\n",
        "param_groups = [{'params': regular_params},\n",
        "                {'params': galore_params, 'rank': 128, 'update_proj_gap': 50, 'scale': 1, 'proj_type': \"std\"}]\n",
        "\n",
        "\n",
        "optimizer = GaLoreAdamW(param_groups, lr=1e-5, weight_decay=0.0)\n",
        ""
      ],
      "metadata": {
        "id": "gZo8Dzukfj1S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVD Galore From Scratch"
      ],
      "metadata": {
        "id": "5gdXdwtymbU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy dependencies from transformers/optimization.py\n",
        "import math\n",
        "import warnings\n",
        "from typing import Callable, Iterable, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "# from .galore_projector import GaLoreProjector\n",
        "# from .galore_projector_tensor import GaLoreProjectorTensor\n",
        "import torch\n",
        "\n",
        "class GaLoreProjector:\n",
        "    def __init__(self, rank, verbose=False, update_proj_gap=200, scale=1.0, proj_type='std'):\n",
        "        self.rank = rank\n",
        "        self.verbose = verbose\n",
        "        self.update_proj_gap = update_proj_gap\n",
        "        self.scale = scale\n",
        "        self.ortho_matrix = None\n",
        "        self.proj_type = proj_type\n",
        "\n",
        "    def project(self, full_rank_grad, iter):\n",
        "        if self.proj_type == 'std':\n",
        "            if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "                low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "            else:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "                low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type), full_rank_grad)\n",
        "        elif self.proj_type == 'reverse_std':\n",
        "            if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "                low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type),full_rank_grad)\n",
        "            else:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "                low_rank_grad = torch.matmul(full_rank_grad,self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "        elif self.proj_type == 'right':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "            low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "        elif self.proj_type == 'left':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "            low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type), full_rank_grad)\n",
        "        elif self.proj_type == 'full':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='full')\n",
        "            low_rank_grad = torch.matmul(self.ortho_matrix[0].t().to(full_rank_grad.device.type), full_rank_grad) @ self.ortho_matrix[1].t().to(full_rank_grad.device.type)\n",
        "\n",
        "        return low_rank_grad\n",
        "\n",
        "    def project_back(self, low_rank_grad):\n",
        "        if self.proj_type == 'std':\n",
        "            if low_rank_grad.shape[0] >= low_rank_grad.shape[1]:\n",
        "                full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "            else:\n",
        "                full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "        elif self.proj_type == 'reverse_std':\n",
        "            if low_rank_grad.shape[0] <= low_rank_grad.shape[1]: # note this is different from std\n",
        "                full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "            else:\n",
        "                full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "        elif self.proj_type == 'right':\n",
        "            full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "        elif self.proj_type == 'left':\n",
        "            full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "        elif self.proj_type == 'full':\n",
        "            full_rank_grad = torch.matmul(self.ortho_matrix[0].to(low_rank_grad.device.type), low_rank_grad) @ self.ortho_matrix[1].to(low_rank_grad.device.type)\n",
        "\n",
        "\n",
        "        return full_rank_grad * self.scale\n",
        "\n",
        "\n",
        "    # svd decomposition\n",
        "    def get_orthogonal_matrix(self, weights, rank, type):\n",
        "        module_params = weights\n",
        "\n",
        "        if module_params.data.dtype != torch.float:\n",
        "            float_data = False\n",
        "            original_type = module_params.data.dtype\n",
        "            original_device = module_params.data.device\n",
        "            matrix = module_params.data.float()\n",
        "        else:\n",
        "            float_data = True\n",
        "            matrix = module_params.data\n",
        "\n",
        "        U, s, Vh = torch.linalg.svd(matrix, full_matrices = False)\n",
        "\n",
        "        #make the smaller matrix always to be orthogonal matrix\n",
        "        if type=='right':\n",
        "            B = Vh[:rank, :]\n",
        "            if not float_data:\n",
        "                B = B.to(original_device).type(original_type)\n",
        "            return B\n",
        "        elif type=='left':\n",
        "            A = U[:, :rank]\n",
        "            if not float_data:\n",
        "                A = A.to(original_device).type(original_type)\n",
        "            return A\n",
        "        elif type=='full':\n",
        "            A = U[:, :rank]\n",
        "            B = Vh[:rank, :]\n",
        "            if not float_data:\n",
        "                A = A.to(original_device).type(original_type)\n",
        "                B = B.to(original_device).type(original_type)\n",
        "            return [A, B]\n",
        "        else:\n",
        "            raise ValueError('type should be left, right or full')\n",
        "\n",
        "\n",
        "import torch\n",
        "from tensorly.decomposition import tucker\n",
        "from tensorly import tenalg\n",
        "\n",
        "# The GaLoreProjector class in Python implements a projection method using orthogonal matrix\n",
        "# decomposition for low-rank approximation of gradients for general tensors of dimension >2.\n",
        "# We use tensor decomposition using tensorly library: https://tensorly.org/stable/index.html\n",
        "class GaLoreProjectorTensor:\n",
        "    \"\"\"\n",
        "    A class that represents a projector for the GaLore algorithm.\n",
        "\n",
        "    Args:\n",
        "        rank (int): The rank of the projector.\n",
        "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
        "        update_proj_gap (int, optional): The number of iterations between updating the orthogonal matrix. Defaults to 200.\n",
        "        scale (float, optional): The scaling factor for the projected gradients. Defaults to 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rank, verbose=False, update_proj_gap=200, scale=1.0):\n",
        "        self.rank = rank\n",
        "        self.verbose = verbose\n",
        "        self.update_proj_gap = update_proj_gap\n",
        "        self.scale = scale\n",
        "        self.ortho_matrix = None\n",
        "        self.transformed_low_rank = None\n",
        "\n",
        "    def project(self, full_rank_grad, iter):\n",
        "        \"\"\"\n",
        "        Projects the full-rank gradients onto the low-rank subspace.\n",
        "\n",
        "        Args:\n",
        "            full_rank_grad (torch.Tensor): The full-rank gradients.\n",
        "            iter (int): The current iteration.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The transformed low-rank gradients.\n",
        "        \"\"\"\n",
        "        if self.ortho_matrix is None and iter % self.update_proj_gap == 0:\n",
        "            self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank)\n",
        "        self.transformed_low_rank = self.transform(self.ortho_matrix, full_rank_grad)\n",
        "        return self.transformed_low_rank\n",
        "\n",
        "    def project_back(self, low_rank_grad):\n",
        "        \"\"\"\n",
        "        Projects the low-rank gradients back to the full-rank space.\n",
        "\n",
        "        Args:\n",
        "            low_rank_grad (torch.Tensor): The low-rank gradients.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The full-rank gradients.\n",
        "        \"\"\"\n",
        "        full_rank_grad = self.inverse_transform(self.ortho_matrix, self.transformed_low_rank)\n",
        "        return full_rank_grad * self.scale\n",
        "\n",
        "    # svd decomposition\n",
        "    def get_orthogonal_matrix(self, weights, rank_all):\n",
        "        \"\"\"\n",
        "        Computes the orthogonal matrix using SVD decomposition.\n",
        "\n",
        "        Args:\n",
        "            weights (torch.Tensor): The weights to decompose.\n",
        "            rank_all (int): The desired rank of the decomposition.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the core and factors of the orthogonal matrix.\n",
        "        \"\"\"\n",
        "        module_params = weights\n",
        "        if module_params.data.dtype != torch.float:\n",
        "            matrix = module_params.data.float()\n",
        "        else:\n",
        "            matrix = module_params.data\n",
        "        tucker_tensor = tucker(matrix, rank=rank_all)\n",
        "        return tucker_tensor\n",
        "\n",
        "    def transform(self, tensor, x):\n",
        "        \"\"\"\n",
        "        Transforms the input tensor using the factors of the orthogonal matrix.\n",
        "\n",
        "        Args:\n",
        "            tensor (tuple): A tuple containing the core and factors of the orthogonal matrix.\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The transformed tensor.\n",
        "        \"\"\"\n",
        "        _, factors = tensor\n",
        "        return tenalg.multi_mode_dot(x, factors, transpose=True)\n",
        "\n",
        "    def inverse_transform(self, tensor, x):\n",
        "        \"\"\"\n",
        "        Inverse transforms the input tensor using the factors of the orthogonal matrix.\n",
        "\n",
        "        Args:\n",
        "            tensor (tuple): A tuple containing the core and factors of the orthogonal matrix.\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The inverse transformed tensor.\n",
        "        \"\"\"\n",
        "        _, factors = tensor\n",
        "        return tenalg.multi_mode_dot(x, factors)\n",
        "\n",
        "\n",
        "\n",
        "class GaLoreAdamW(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
        "    Regularization](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "    Parameters:\n",
        "        params (`Iterable[nn.parameter.Parameter]`):\n",
        "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
        "        lr (`float`, *optional*, defaults to 0.001):\n",
        "            The learning rate to use.\n",
        "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n",
        "            Adam's betas parameters (b1, b2).\n",
        "        eps (`float`, *optional*, defaults to 1e-06):\n",
        "            Adam's epsilon for numerical stability.\n",
        "        weight_decay (`float`, *optional*, defaults to 0.0):\n",
        "            Decoupled weight decay to apply.\n",
        "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
        "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
        "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[nn.parameter.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        correct_bias: bool = True,\n",
        "        no_deprecation_warning: bool = False,\n",
        "    ):\n",
        "        if not no_deprecation_warning:\n",
        "            warnings.warn(\n",
        "                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"\n",
        "                \" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"\n",
        "                \" warning\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure: Callable = None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if \"step\" not in state:\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                if 'dim' not in group:\n",
        "                    group['dim'] = 2\n",
        "\n",
        "                # GaLore Projection\n",
        "                if \"rank\" in group:\n",
        "                    if \"projector\" not in state:\n",
        "                        if group['dim'] <=2:\n",
        "                            state[\"projector\"] = GaLoreProjector(group[\"rank\"], update_proj_gap=group[\"update_proj_gap\"], scale=group[\"scale\"], proj_type=group[\"proj_type\"])\n",
        "                        else:\n",
        "                            state[\"projector\"] = GaLoreProjectorTensor(group[\"rank\"], update_proj_gap=group[\"update_proj_gap\"], scale=group[\"scale\"], proj_type=group[\"proj_type\"])\n",
        "                    grad = state[\"projector\"].project(grad, state[\"step\"])\n",
        "\n",
        "                # State initialization\n",
        "                if \"exp_avg\" not in state:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(grad)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
        "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "\n",
        "                step_size = group[\"lr\"]\n",
        "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
        "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
        "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
        "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                # compute norm gradient\n",
        "                norm_grad = exp_avg / denom\n",
        "\n",
        "                # GaLore Projection Back\n",
        "                if \"rank\" in group:\n",
        "                    norm_grad = state[\"projector\"].project_back(norm_grad)\n",
        "\n",
        "                p.add_(norm_grad, alpha=-step_size)\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                # Add weight decay at the end (fixed version)\n",
        "                if group[\"weight_decay\"] > 0.0:\n",
        "                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "OGsVmQHxlvqQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply SVD Galore"
      ],
      "metadata": {
        "id": "CgtekKRmnxTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 (better than standard FP4)\n",
        "    bnb_4bit_use_double_quant=True,  # Uses secondary quantization for better precision\n",
        "    bnb_4bit_compute_dtype=torch.float16  # Keeps computation in FP16 for stability\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "# from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor\n",
        "\n",
        "# make parameters with \"rank\" to a single group, if param_name has \"mlp\" or \"attn\"\n",
        "galore_params = []\n",
        "target_modules_list = [\"attn\", \"mlp\"]\n",
        "for module_name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "\n",
        "    if not any(target_key in module_name for target_key in target_modules_list):\n",
        "        continue\n",
        "\n",
        "    # print('enable GaLore for weights in module: ', module_name)\n",
        "    galore_params.append(module.weight)\n",
        "id_galore_params = [id(p) for p in galore_params]\n",
        "# make parameters without \"rank\" to another group\n",
        "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
        "# then call galore_adamw\n",
        "param_groups = [{'params': regular_params},\n",
        "                {'params': galore_params, 'rank': 128, 'update_proj_gap': 50, 'scale': 1, 'proj_type': \"std\"}]\n",
        "\n",
        "\n",
        "optimizer = GaLoreAdamW(param_groups, lr=1e-5, weight_decay=0.0)"
      ],
      "metadata": {
        "id": "GVd5c8Dcm_fY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Truncated-SVD Galore From Scratch"
      ],
      "metadata": {
        "id": "kAH3VajbnaN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy dependencies from transformers/optimization.py\n",
        "import math\n",
        "import warnings\n",
        "from typing import Callable, Iterable, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "# from .galore_projector import GaLoreProjector\n",
        "# from .galore_projector_tensor import GaLoreProjectorTensor\n",
        "import torch\n",
        "\n",
        "class GaLoreProjector:\n",
        "    def __init__(self, rank, verbose=False, update_proj_gap=200, scale=1.0, proj_type='std'):\n",
        "        self.rank = rank\n",
        "        self.verbose = verbose\n",
        "        self.update_proj_gap = update_proj_gap\n",
        "        self.scale = scale\n",
        "        self.ortho_matrix = None\n",
        "        self.proj_type = proj_type\n",
        "\n",
        "    def project(self, full_rank_grad, iter):\n",
        "        if self.proj_type == 'std':\n",
        "            if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "                low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "            else:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "                low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type), full_rank_grad)\n",
        "        elif self.proj_type == 'reverse_std':\n",
        "            if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "                low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type),full_rank_grad)\n",
        "            else:\n",
        "                if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "                low_rank_grad = torch.matmul(full_rank_grad,self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "        elif self.proj_type == 'right':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')\n",
        "            low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t().to(full_rank_grad.device.type))\n",
        "        elif self.proj_type == 'left':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')\n",
        "            low_rank_grad = torch.matmul(self.ortho_matrix.t().to(full_rank_grad.device.type), full_rank_grad)\n",
        "        elif self.proj_type == 'full':\n",
        "            if self.ortho_matrix is None or iter % self.update_proj_gap == 0:\n",
        "                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='full')\n",
        "            low_rank_grad = torch.matmul(self.ortho_matrix[0].t().to(full_rank_grad.device.type), full_rank_grad) @ self.ortho_matrix[1].t().to(full_rank_grad.device.type)\n",
        "\n",
        "        return low_rank_grad\n",
        "\n",
        "    def project_back(self, low_rank_grad):\n",
        "        if self.proj_type == 'std':\n",
        "            if low_rank_grad.shape[0] >= low_rank_grad.shape[1]:\n",
        "                full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "            else:\n",
        "                full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "        elif self.proj_type == 'reverse_std':\n",
        "            if low_rank_grad.shape[0] <= low_rank_grad.shape[1]: # note this is different from std\n",
        "                full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "            else:\n",
        "                full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "        elif self.proj_type == 'right':\n",
        "            full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix.to(low_rank_grad.device.type))\n",
        "        elif self.proj_type == 'left':\n",
        "            full_rank_grad = torch.matmul(self.ortho_matrix.to(low_rank_grad.device.type), low_rank_grad)\n",
        "        elif self.proj_type == 'full':\n",
        "            full_rank_grad = torch.matmul(self.ortho_matrix[0].to(low_rank_grad.device.type), low_rank_grad) @ self.ortho_matrix[1].to(low_rank_grad.device.type)\n",
        "\n",
        "\n",
        "        return full_rank_grad * self.scale\n",
        "\n",
        "\n",
        "    # svd decomposition\n",
        "    def get_orthogonal_matrix(self, weights, rank, type):\n",
        "        module_params = weights\n",
        "\n",
        "        if module_params.data.dtype != torch.float:\n",
        "            float_data = False\n",
        "            original_type = module_params.data.dtype\n",
        "            original_device = module_params.data.device\n",
        "            matrix = module_params.data.float()\n",
        "        else:\n",
        "            float_data = True\n",
        "            matrix = module_params.data\n",
        "\n",
        "        # U, s, Vh = torch.linalg.svd(matrix, full_matrices = False)\n",
        "        U, S, Vh = torch.svd_lowrank(A, q=rank) # mobarak\n",
        "\n",
        "        #make the smaller matrix always to be orthogonal matrix\n",
        "        if type=='right':\n",
        "            B = Vh[:rank, :]\n",
        "            if not float_data:\n",
        "                B = B.to(original_device).type(original_type)\n",
        "            return B\n",
        "        elif type=='left':\n",
        "            A = U[:, :rank]\n",
        "            if not float_data:\n",
        "                A = A.to(original_device).type(original_type)\n",
        "            return A\n",
        "        elif type=='full':\n",
        "            A = U[:, :rank]\n",
        "            B = Vh[:rank, :]\n",
        "            if not float_data:\n",
        "                A = A.to(original_device).type(original_type)\n",
        "                B = B.to(original_device).type(original_type)\n",
        "            return [A, B]\n",
        "        else:\n",
        "            raise ValueError('type should be left, right or full')\n",
        "\n",
        "\n",
        "import torch\n",
        "from tensorly.decomposition import tucker\n",
        "from tensorly import tenalg\n",
        "\n",
        "# The GaLoreProjector class in Python implements a projection method using orthogonal matrix\n",
        "# decomposition for low-rank approximation of gradients for general tensors of dimension >2.\n",
        "# We use tensor decomposition using tensorly library: https://tensorly.org/stable/index.html\n",
        "class GaLoreProjectorTensor:\n",
        "    \"\"\"\n",
        "    A class that represents a projector for the GaLore algorithm.\n",
        "\n",
        "    Args:\n",
        "        rank (int): The rank of the projector.\n",
        "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
        "        update_proj_gap (int, optional): The number of iterations between updating the orthogonal matrix. Defaults to 200.\n",
        "        scale (float, optional): The scaling factor for the projected gradients. Defaults to 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rank, verbose=False, update_proj_gap=200, scale=1.0):\n",
        "        self.rank = rank\n",
        "        self.verbose = verbose\n",
        "        self.update_proj_gap = update_proj_gap\n",
        "        self.scale = scale\n",
        "        self.ortho_matrix = None\n",
        "        self.transformed_low_rank = None\n",
        "\n",
        "    def project(self, full_rank_grad, iter):\n",
        "        \"\"\"\n",
        "        Projects the full-rank gradients onto the low-rank subspace.\n",
        "\n",
        "        Args:\n",
        "            full_rank_grad (torch.Tensor): The full-rank gradients.\n",
        "            iter (int): The current iteration.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The transformed low-rank gradients.\n",
        "        \"\"\"\n",
        "        if self.ortho_matrix is None and iter % self.update_proj_gap == 0:\n",
        "            self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank)\n",
        "        self.transformed_low_rank = self.transform(self.ortho_matrix, full_rank_grad)\n",
        "        return self.transformed_low_rank\n",
        "\n",
        "    def project_back(self, low_rank_grad):\n",
        "        \"\"\"\n",
        "        Projects the low-rank gradients back to the full-rank space.\n",
        "\n",
        "        Args:\n",
        "            low_rank_grad (torch.Tensor): The low-rank gradients.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The full-rank gradients.\n",
        "        \"\"\"\n",
        "        full_rank_grad = self.inverse_transform(self.ortho_matrix, self.transformed_low_rank)\n",
        "        return full_rank_grad * self.scale\n",
        "\n",
        "    # svd decomposition\n",
        "    def get_orthogonal_matrix(self, weights, rank_all):\n",
        "        \"\"\"\n",
        "        Computes the orthogonal matrix using SVD decomposition.\n",
        "\n",
        "        Args:\n",
        "            weights (torch.Tensor): The weights to decompose.\n",
        "            rank_all (int): The desired rank of the decomposition.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the core and factors of the orthogonal matrix.\n",
        "        \"\"\"\n",
        "        module_params = weights\n",
        "        if module_params.data.dtype != torch.float:\n",
        "            matrix = module_params.data.float()\n",
        "        else:\n",
        "            matrix = module_params.data\n",
        "        tucker_tensor = tucker(matrix, rank=rank_all)\n",
        "        return tucker_tensor\n",
        "\n",
        "    def transform(self, tensor, x):\n",
        "        \"\"\"\n",
        "        Transforms the input tensor using the factors of the orthogonal matrix.\n",
        "\n",
        "        Args:\n",
        "            tensor (tuple): A tuple containing the core and factors of the orthogonal matrix.\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The transformed tensor.\n",
        "        \"\"\"\n",
        "        _, factors = tensor\n",
        "        return tenalg.multi_mode_dot(x, factors, transpose=True)\n",
        "\n",
        "    def inverse_transform(self, tensor, x):\n",
        "        \"\"\"\n",
        "        Inverse transforms the input tensor using the factors of the orthogonal matrix.\n",
        "\n",
        "        Args:\n",
        "            tensor (tuple): A tuple containing the core and factors of the orthogonal matrix.\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The inverse transformed tensor.\n",
        "        \"\"\"\n",
        "        _, factors = tensor\n",
        "        return tenalg.multi_mode_dot(x, factors)\n",
        "\n",
        "\n",
        "\n",
        "class Truncated_GaLoreAdamW(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
        "    Regularization](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "    Parameters:\n",
        "        params (`Iterable[nn.parameter.Parameter]`):\n",
        "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
        "        lr (`float`, *optional*, defaults to 0.001):\n",
        "            The learning rate to use.\n",
        "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n",
        "            Adam's betas parameters (b1, b2).\n",
        "        eps (`float`, *optional*, defaults to 1e-06):\n",
        "            Adam's epsilon for numerical stability.\n",
        "        weight_decay (`float`, *optional*, defaults to 0.0):\n",
        "            Decoupled weight decay to apply.\n",
        "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
        "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
        "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[nn.parameter.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        correct_bias: bool = True,\n",
        "        no_deprecation_warning: bool = False,\n",
        "    ):\n",
        "        if not no_deprecation_warning:\n",
        "            warnings.warn(\n",
        "                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"\n",
        "                \" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"\n",
        "                \" warning\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure: Callable = None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if \"step\" not in state:\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                if 'dim' not in group:\n",
        "                    group['dim'] = 2\n",
        "\n",
        "                # GaLore Projection\n",
        "                if \"rank\" in group:\n",
        "                    if \"projector\" not in state:\n",
        "                        if group['dim'] <=2:\n",
        "                            state[\"projector\"] = GaLoreProjector(group[\"rank\"], update_proj_gap=group[\"update_proj_gap\"], scale=group[\"scale\"], proj_type=group[\"proj_type\"])\n",
        "                        else:\n",
        "                            state[\"projector\"] = GaLoreProjectorTensor(group[\"rank\"], update_proj_gap=group[\"update_proj_gap\"], scale=group[\"scale\"], proj_type=group[\"proj_type\"])\n",
        "                    grad = state[\"projector\"].project(grad, state[\"step\"])\n",
        "\n",
        "                # State initialization\n",
        "                if \"exp_avg\" not in state:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(grad)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
        "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "\n",
        "                step_size = group[\"lr\"]\n",
        "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
        "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
        "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
        "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                # compute norm gradient\n",
        "                norm_grad = exp_avg / denom\n",
        "\n",
        "                # GaLore Projection Back\n",
        "                if \"rank\" in group:\n",
        "                    norm_grad = state[\"projector\"].project_back(norm_grad)\n",
        "\n",
        "                p.add_(norm_grad, alpha=-step_size)\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                # Add weight decay at the end (fixed version)\n",
        "                if group[\"weight_decay\"] > 0.0:\n",
        "                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "6ShXrpY4nHLx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply truncated-svd galore"
      ],
      "metadata": {
        "id": "-Pn6Xle-oERK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 (better than standard FP4)\n",
        "    bnb_4bit_use_double_quant=True,  # Uses secondary quantization for better precision\n",
        "    bnb_4bit_compute_dtype=torch.float16  # Keeps computation in FP16 for stability\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "# from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor\n",
        "\n",
        "# make parameters with \"rank\" to a single group, if param_name has \"mlp\" or \"attn\"\n",
        "galore_params = []\n",
        "target_modules_list = [\"attn\", \"mlp\"]\n",
        "for module_name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "\n",
        "    if not any(target_key in module_name for target_key in target_modules_list):\n",
        "        continue\n",
        "\n",
        "    # print('enable GaLore for weights in module: ', module_name)\n",
        "    galore_params.append(module.weight)\n",
        "id_galore_params = [id(p) for p in galore_params]\n",
        "# make parameters without \"rank\" to another group\n",
        "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
        "# then call galore_adamw\n",
        "param_groups = [{'params': regular_params},\n",
        "                {'params': galore_params, 'rank': 128, 'update_proj_gap': 50, 'scale': 1, 'proj_type': \"std\"}]\n",
        "\n",
        "\n",
        "optimizer = Truncated_GaLoreAdamW(param_groups, lr=1e-5, weight_decay=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCCEkwPSoDc_",
        "outputId": "80f12fbb-204a-4322-cd1c-f1282651dee3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e5870bd4efc5>:257: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zahr3wd4ocu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}