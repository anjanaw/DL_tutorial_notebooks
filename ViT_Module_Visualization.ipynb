{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/ViT_Module_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViTs Variations\n",
        "paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (https://arxiv.org/pdf/2010.11929.pdf)<br>\n",
        "src1: https://colab.research.google.com/github/hirotomusiker/schwert_colab_data_storage/blob/master/notebook/Vision_Transformer_Tutorial.ipynb<br>\n",
        "\n",
        "src2: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html"
      ],
      "metadata": {
        "id": "e4xOyzpVXv9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm pandas requests"
      ],
      "metadata": {
        "id": "1YoBxauQTxxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from timm import create_model\n",
        "\n",
        "model_name = \"vit_base_patch16_224\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device = \", device)\n",
        "# create a ViT model : https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "model = create_model(model_name, pretrained=True).to(device)\n",
        "\n",
        "# Define transforms for test\n",
        "IMG_SIZE = (224, 224)\n",
        "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
        "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
        "transforms = [\n",
        "              T.Resize(IMG_SIZE),\n",
        "              T.ToTensor(),\n",
        "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
        "              ]\n",
        "\n",
        "transforms = T.Compose(transforms)\n",
        "\n",
        "# capture\n",
        "# ImageNet Labels\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
        "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))\n",
        "\n",
        "# Demo Image\n",
        "!wget https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/santorini.png?raw=true -O santorini.png\n",
        "img = PIL.Image.open('santorini.png')\n",
        "img_tensor = transforms(img).unsqueeze(0).to(device)\n",
        "\n",
        "# end-to-end inference\n",
        "output = model(img_tensor)\n",
        "\n",
        "print(\"Inference Result:\")\n",
        "print(imagenet_labels[int(torch.argmax(output))])\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "rEoqRfpKX0gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dig into Vision Transformer\n",
        "\n",
        "<img src='https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/vit_input.png?raw=true'>\n",
        "\n",
        "Figure 1. Vision Transformer inference pipeline.  \n",
        "1. Split Image into Patches  \n",
        "The input image is split into 14 x 14 vectors with dimension of 768 by Conv2d (k=16x16) with stride=(16, 16).\n",
        "2. Add Position Embeddings  \n",
        "Learnable position embedding vectors are added to the patch embedding vectors and fed to the transformer encoder.\n",
        "3. Transformer Encoder  \n",
        "The embedding vectors are encoded by the transformer encoder. The dimension of input and output vectors are the same. Details of the encoder are depicted in Fig. 2.\n",
        "4. MLP (Classification) Head  \n",
        "The 0th output from the encoder is fed to the MLP head for classification to output the final classification results."
      ],
      "metadata": {
        "id": "dmEOiMhFhOY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Split Image into Patches"
      ],
      "metadata": {
        "id": "fXK6EBrBiY3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patches = model.patch_embed(img_tensor)  # patch embedding convolution\n",
        "print(\"Image tensor: \", img_tensor.shape)\n",
        "print(\"Patch embeddings: \", patches.shape)\n",
        "\n",
        "# This is NOT a part of the pipeline.\n",
        "# Actually the image is divided into patch embeddings by Conv2d\n",
        "# with stride=(16, 16) shown above.\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "fig.suptitle(\"Visualization of Patches\", fontsize=24)\n",
        "rect = 0.15, 0.1, 0.4, 0.1 #left, bottom, width, height)\n",
        "fig.add_axes(rect)\n",
        "plt.axis('off')\n",
        "img = np.asarray(img)\n",
        "for i in range(0, 196):\n",
        "    x = i % 14\n",
        "    y = i // 14\n",
        "    patch = img[y*16:(y+1)*16, x*16:(x+1)*16]\n",
        "    ax = fig.add_subplot(14, 14, i+1)\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.imshow(patch)"
      ],
      "metadata": {
        "id": "nVcgDwSMh4BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Add Position Embeddings"
      ],
      "metadata": {
        "id": "yGf4ctLoiaS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed = model.pos_embed\n",
        "print(pos_embed.shape)\n",
        "\n",
        "# Visualize position embedding similarities.\n",
        "# One cell shows cos similarity between an embedding and all the other embeddings.\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "fig.suptitle(\"Visualization of position embedding similarities\", fontsize=24)\n",
        "for i in range(1, pos_embed.shape[1]):\n",
        "    sim = F.cosine_similarity(pos_embed[0, i:i+1], pos_embed[0, 1:], dim=1)\n",
        "    sim = sim.reshape((14, 14)).detach().cpu().numpy()\n",
        "    ax = fig.add_subplot(14, 14, i)\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.imshow(sim)"
      ],
      "metadata": {
        "id": "sbb0DnbDidMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Transformer Input:<br>\n",
        "A learnable class token is prepended to the patch embedding vectors as the 0th vector.\n",
        "197 (1 + 14 x 14) learnable position embedding vectors are added to the patch embedding vectors."
      ],
      "metadata": {
        "id": "sHHCn1Uwi2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_input = torch.cat((model.cls_token, patches), dim=1) + pos_embed\n",
        "print(\"Transformer input: \", transformer_input.shape)"
      ],
      "metadata": {
        "id": "9mwA_t_5jBsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transformer Encoder\n",
        "<img src='https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/transformer_encoder.png?raw=true'>\n",
        "\n",
        "Figure 2. Detailed schematic of Transformer Encoder.\n",
        "- N (=197) embedded vectors are fed to the L (=12) series encoders.\n",
        "- The vectors are divided into query, key and value after expanded by an fc layer.\n",
        "- q, k and v are further divided into H (=12) and fed to the parallel attention heads.\n",
        "- Outputs from attention heads are concatenated to form the vectors whose shape is the same as the encoder input.\n",
        "- The vectors go through an fc, a layer norm and an MLP block that has two fc layers.\n",
        "\n",
        "The Vision Transformer employs the Transformer Encoder that was proposed in the [attention is all you need paper](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
        "\n",
        "Implementation Reference:\n",
        "\n",
        "- [tensorflow implementation](https://github.com/google-research/vision_transformer/blob/502746cb287a107f9911c061f9d9c2c0159c81cc/vit_jax/models.py#L62-L146)\n",
        "- [pytorch implementation (timm)](https://github.com/rwightman/pytorch-image-models/blob/198f6ea0f3dae13f041f3ea5880dd79089b60d61/timm/models/vision_transformer.py#L79-L143)"
      ],
      "metadata": {
        "id": "Ty3WTT22je5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Series Transformer Encoders\n",
        "print(\"Input tensor to Transformer (z0): \", transformer_input.shape)\n",
        "x = transformer_input.clone()\n",
        "for i, blk in enumerate(model.blocks):\n",
        "    print(\"Entering the Transformer Encoder {}, input:{}\".format(i, x.shape))\n",
        "    x = blk(x)\n",
        "x = model.norm(x)\n",
        "transformer_output = x[:, 0]\n",
        "print(\"Output vector from Transformer (z12-0):\", transformer_output.shape)"
      ],
      "metadata": {
        "id": "QIhcKCBcjouf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Attention Works"
      ],
      "metadata": {
        "id": "8WDZ1tHvl6H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transformer Multi-head Attention block:\")\n",
        "attention = model.blocks[0].attn\n",
        "print(attention)\n",
        "print(\"input of the transformer encoder:\", transformer_input.shape)\n",
        "\n",
        "# fc layer to expand the dimension\n",
        "transformer_input_expanded = attention.qkv(transformer_input)[0]\n",
        "print(\"expanded to: \", transformer_input_expanded.shape)\n",
        "\n",
        "# Split qkv into mulitple q, k, and v vectors for multi-head attantion\n",
        "qkv = transformer_input_expanded.reshape(197, 3, 12, 64)  # (N=197, (qkv), H=12, D/H=64)\n",
        "print(\"split qkv : \", qkv.shape)\n",
        "q = qkv[:, 0].permute(1, 0, 2)  # (H=12, N=197, D/H=64)\n",
        "k = qkv[:, 1].permute(1, 0, 2)  # (H=12, N=197, D/H=64)\n",
        "kT = k.permute(0, 2, 1)  # (H=12, D/H=64, N=197)\n",
        "print(\"transposed ks: \", kT.shape)\n",
        "\n",
        "# Attention Matrix\n",
        "attention_matrix = q @ kT\n",
        "print(\"attention matrix: \", attention_matrix.shape)\n",
        "plt.imshow(attention_matrix[3].detach().cpu().numpy())\n",
        "\n",
        "# Visualize attention matrix\n",
        "fig = plt.figure(figsize=(16, 8))\n",
        "fig.suptitle(\"Visualization of Attention\", fontsize=24)\n",
        "fig.add_axes()\n",
        "img = np.asarray(img)\n",
        "ax = fig.add_subplot(2, 4, 1)\n",
        "ax.imshow(img)\n",
        "for i in range(7):  # visualize the 100th rows of attention matrices in the 0-7th heads\n",
        "    attn_heatmap = attention_matrix[i, 100, 1:].reshape((14, 14)).detach().cpu().numpy()\n",
        "    ax = fig.add_subplot(2, 4, i+2)\n",
        "    ax.imshow(attn_heatmap)"
      ],
      "metadata": {
        "id": "vEebtJKCl7lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. MLP (Classification) Head"
      ],
      "metadata": {
        "id": "O0ww8PtUmtwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification head: \", model.head)\n",
        "result = model.head(transformer_output)\n",
        "result_label_id = int(torch.argmax(result))\n",
        "plt.plot(result.detach().cpu().numpy()[0])\n",
        "plt.title(\"Classification result\")\n",
        "plt.xlabel(\"class id\")\n",
        "print(\"Inference result : id = {}, label name = {}\".format(\n",
        "    result_label_id, imagenet_labels[result_label_id]))"
      ],
      "metadata": {
        "id": "KVS8pkXgmxds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViTs from https://github.com/jeonsworld/ViT-pytorch"
      ],
      "metadata": {
        "id": "Tz3UM-sE7fcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/jeonsworld/ViT-pytorch\n",
        "%cd ViT-pytorch"
      ],
      "metadata": {
        "id": "KVvX6igt7nZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ml_collections"
      ],
      "metadata": {
        "id": "JCLsBxN_77Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "import io\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlretrieve\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "\n",
        "os.makedirs(\"attention_data\", exist_ok=True)\n",
        "if not os.path.isfile(\"attention_data/ilsvrc2012_wordnet_lemmas.txt\"):\n",
        "    urlretrieve(\"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\", \"attention_data/ilsvrc2012_wordnet_lemmas.txt\")\n",
        "if not os.path.isfile(\"attention_data/ViT-B_16-224.npz\"):\n",
        "    urlretrieve(\"https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\", \"attention_data/ViT-B_16-224.npz\")\n",
        "\n",
        "imagenet_labels = dict(enumerate(open('attention_data/ilsvrc2012_wordnet_lemmas.txt')))\n",
        "\n",
        "\n",
        "# Test Image\n",
        "#img_url = 'https://www.purina.co.uk/sites/default/files/styles/square_medium_440x440/public/2022-08/Welsh-Corgi-Pembroke.jpg'\n",
        "img_url = 'https://www.animalfunfacts.net/images/stories/pets/dogs/pembroke_welsh_corgi_l.jpg'\n",
        "urlretrieve(img_url, \"attention_data/img.jpg\")\n",
        "\n",
        "# Prepare Model\n",
        "config = CONFIGS[\"ViT-B_16\"]\n",
        "model = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True)\n",
        "model.load_from(np.load(\"attention_data/ViT-B_16-224.npz\"))\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "im = Image.open(\"attention_data/img.jpg\")\n",
        "x = transform(im)\n",
        "print(x.size())\n",
        "\n",
        "logits, att_mat = model(x.unsqueeze(0))\n",
        "\n",
        "att_mat = torch.stack(att_mat).squeeze(1)\n",
        "# Average the attention weights across all heads.\n",
        "att_mat = torch.mean(att_mat, dim=1)\n",
        "# To account for residual connections, we add an identity matrix to the\n",
        "# attention matrix and re-normalize the weights.\n",
        "residual_att = torch.eye(att_mat.size(1))\n",
        "aug_att_mat = att_mat + residual_att\n",
        "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "# Recursively multiply the weight matrices\n",
        "joint_attentions = torch.zeros(aug_att_mat.size())\n",
        "joint_attentions[0] = aug_att_mat[0]\n",
        "\n",
        "for n in range(1, aug_att_mat.size(0)):\n",
        "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
        "\n",
        "# Attention from the output token to the input space.\n",
        "v = joint_attentions[-1]\n",
        "print('mean att_mat + residual_att + sum + Multi:',v.shape)\n",
        "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
        "print('grid_size:', grid_size, v[0, 1:].shape)\n",
        "mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
        "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
        "print(mask.shape, np.array(im).shape, mask.max(), np.array(im).max())\n",
        "result = (mask * im).astype(\"uint8\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "\n",
        "ax1.set_title('Original')\n",
        "ax2.set_title('Attention Map')\n",
        "_ = ax1.imshow(im)\n",
        "_ = ax2.imshow(result)\n",
        "\n",
        "probs = torch.nn.Softmax(dim=-1)(logits)\n",
        "top5 = torch.argsort(probs, dim=-1, descending=True)\n",
        "print(\"Prediction Label and Attention Map!\\n\")\n",
        "for idx in top5[0, :5]:\n",
        "    print(f'{probs[0, idx.item()]:.5f} : {imagenet_labels[idx.item()]}', end='')"
      ],
      "metadata": {
        "id": "T88FqNDk7z2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "eCqaOW5sHqhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying Images with DeiT"
      ],
      "metadata": {
        "id": "8xva-JbYTvVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEzl7nKmTTzQ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import timm\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "print(torch.__version__)\n",
        "# should be 1.8.0\n",
        "\n",
        "\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=3),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
        "img = transform(img)[None,]\n",
        "out = model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Weights as TorchScript format:"
      ],
      "metadata": {
        "id": "NWJw1csEUEfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "scripted_model = torch.jit.script(model)\n",
        "scripted_model.save(\"fbdeit_scripted.pt\")"
      ],
      "metadata": {
        "id": "74ZQw8fQTpth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLxvuVFxUMMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}