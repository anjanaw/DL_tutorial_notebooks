{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "219c0d44386c4adbafdedfda182fff0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c05e7e94abeb42feb4b0150acaf254b1",
              "IPY_MODEL_558521606e164f2d9cadf5ba3e776691",
              "IPY_MODEL_efb6a2c4515146da8ebc2650dc0f7c15"
            ],
            "layout": "IPY_MODEL_dc3ce64645dc4167b13ea5420d42ed47"
          }
        },
        "c05e7e94abeb42feb4b0150acaf254b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94a55c386aa4dbe9e1d8d8f959d24d4",
            "placeholder": "​",
            "style": "IPY_MODEL_2d909421238c4c88a4027823d448addd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "558521606e164f2d9cadf5ba3e776691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db8d2bdfd58248a4bd36be5b13ff1cd7",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8365d4bc79334a338e03be513c4363e6",
            "value": 5
          }
        },
        "efb6a2c4515146da8ebc2650dc0f7c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce372f6691ba46ccbf156d7682fbef15",
            "placeholder": "​",
            "style": "IPY_MODEL_cebb17347c9c401a85c1cdbd485827ff",
            "value": " 5/5 [01:08&lt;00:00, 11.83s/it]"
          }
        },
        "dc3ce64645dc4167b13ea5420d42ed47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a94a55c386aa4dbe9e1d8d8f959d24d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d909421238c4c88a4027823d448addd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db8d2bdfd58248a4bd36be5b13ff1cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8365d4bc79334a338e03be513c4363e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce372f6691ba46ccbf156d7682fbef15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebb17347c9c401a85c1cdbd485827ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/qwen_pvqa_train_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch==2.4.0\" tensorboard pillow torchvision accelerate huggingface_hub\n",
        "!pip -q install  --upgrade \\\n",
        "  \"transformers==4.45.1\" \\\n",
        "  \"datasets==3.0.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.44.0\" \\\n",
        "  \"trl==0.11.1\" \\\n",
        "  \"peft==0.13.0\" \\\n",
        "  \"qwen_vl_utils\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO3zAe5zdOXe",
        "outputId": "1fcc1fba-bcc0-4fbb-a24f-260ecf9e20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6YJU5rWc8bn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786,
          "referenced_widgets": [
            "219c0d44386c4adbafdedfda182fff0c",
            "c05e7e94abeb42feb4b0150acaf254b1",
            "558521606e164f2d9cadf5ba3e776691",
            "efb6a2c4515146da8ebc2650dc0f7c15",
            "dc3ce64645dc4167b13ea5420d42ed47",
            "a94a55c386aa4dbe9e1d8d8f959d24d4",
            "2d909421238c4c88a4027823d448addd",
            "db8d2bdfd58248a4bd36be5b13ff1cd7",
            "8365d4bc79334a338e03be513c4363e6",
            "ce372f6691ba46ccbf156d7682fbef15",
            "cebb17347c9c401a85c1cdbd485827ff"
          ]
        },
        "outputId": "39fa5411-7e1d-45e0-b3c6-a5439a3d8809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled Train Dataset: Dataset({\n",
            "    features: ['image', 'question', 'answer'],\n",
            "    num_rows: 100\n",
            "})\n",
            "Sampled Test Dataset: Dataset({\n",
            "    features: ['image', 'question', 'answer'],\n",
            "    num_rows: 100\n",
            "})\n",
            "Sampled Validation Dataset: Dataset({\n",
            "    features: ['image', 'question', 'answer'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "219c0d44386c4adbafdedfda182fff0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,523,136 || all params: 8,293,898,752 || trainable%: 0.0304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmobarakol\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250115_152845-ecgt5ofs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa/runs/ecgt5ofs' target=\"_blank\">qwen2-7b-instruct-trl-sft-pvqa</a></strong> to <a href='https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa' target=\"_blank\">https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa/runs/ecgt5ofs' target=\"_blank\">https://wandb.ai/mobarakol/qwen2-7b-instruct-trl-sft-pvqa/runs/ecgt5ofs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 14:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import wandb\n",
        "from trl import SFTTrainer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "def format_data(sample, system_message):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": sample[\"image\"],\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample['question'],\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[\"answer\"]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "    # Get the texts and images, and apply the chat template\n",
        "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]  # Prepare texts for processing\n",
        "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)  # Encode texts and images into tensors\n",
        "\n",
        "\n",
        "    # for key in batch.keys():\n",
        "    #     batch[key] = batch[key].to(device)\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
        "\n",
        "    # Ignore the image token index in the loss computation (model specific)\n",
        "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
        "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
        "    else:\n",
        "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
        "\n",
        "    # Mask image token IDs in the labels\n",
        "    for image_token_id in image_tokens:\n",
        "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
        "\n",
        "    batch[\"labels\"] = labels  # Add labels to the batch\n",
        "\n",
        "    # print({key: value.device for key, value in batch.items()})\n",
        "\n",
        "    return batch  # Return the prepared batch\n",
        "\n",
        "def sample_subset(dataset_split, n):\n",
        "    return dataset_split.shuffle(seed=42).select(range(min(len(dataset_split), n)))\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from medical images.\n",
        "                Focus on delivering accurate, succinct, short answers based on the visual information.\n",
        "                Avoid additional explanation unless absolutely necessary.\"\"\"\n",
        "\n",
        "\n",
        "    dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
        "    n_samples = 100\n",
        "\n",
        "    # Sample 100 instances from each split\n",
        "    train_subset = sample_subset(dataset[\"train\"], n_samples)\n",
        "    test_subset = sample_subset(dataset[\"test\"], n_samples)\n",
        "    validation_subset = sample_subset(dataset[\"validation\"], n_samples)\n",
        "\n",
        "    # Optional: Save the subsets or print them\n",
        "    print(\"Sampled Train Dataset:\", train_subset)\n",
        "    print(\"Sampled Test Dataset:\", test_subset)\n",
        "    print(\"Sampled Validation Dataset:\", validation_subset)\n",
        "\n",
        "    train_dataset = [format_data(sample, system_message) for sample in train_subset]\n",
        "    eval_dataset = [format_data(sample, system_message) for sample in validation_subset]\n",
        "    test_dataset = [format_data(sample, system_message) for sample in test_subset]\n",
        "\n",
        "\n",
        "    model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "\n",
        "    # BitsAndBytesConfig int-4 config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    # model = model.to(\"cuda\")\n",
        "    processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
        "\n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        r=8,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # Apply PEFT model adaptation\n",
        "    peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # Configure training arguments\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=\"qwen2-7b-instruct-trl-sft-pvqa\",  # Directory to save the model\n",
        "        num_train_epochs=1,  # Number of training epochs\n",
        "        per_device_train_batch_size=4,  # Batch size for training\n",
        "        per_device_eval_batch_size=3,  # Batch size for evaluation\n",
        "        gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
        "        gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
        "        # Optimizer and scheduler settings\n",
        "        optim=\"adamw_torch_fused\",  # Optimizer type\n",
        "        learning_rate=2e-4,  # Learning rate for training\n",
        "        lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
        "        # Logging and evaluation\n",
        "        logging_steps=10,  # Steps interval for logging\n",
        "        eval_steps=10,  # Steps interval for evaluation\n",
        "        eval_strategy=\"steps\",  # Strategy for evaluation\n",
        "        save_strategy=\"steps\",  # Strategy for saving the model\n",
        "        save_steps=20,  # Steps interval for saving\n",
        "        metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
        "        greater_is_better=False,  # Whether higher metric values are better\n",
        "        load_best_model_at_end=True,  # Load the best model after training\n",
        "        # Mixed precision and gradient settings\n",
        "        bf16=True,  # Use bfloat16 precision\n",
        "        # tf32=True,  # Use TensorFloat-32 precision\n",
        "        max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
        "        warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
        "        # Hub and reporting\n",
        "        push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
        "        report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
        "        # Gradient checkpointing settings\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
        "        # Dataset configuration\n",
        "        dataset_text_field=\"\",  # Text field in dataset\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
        "        #max_seq_length=1024  # Maximum sequence length for input\n",
        "        # dataloader_pin_memory=False,  # Disable pin_memory here\n",
        "\n",
        "        )\n",
        "\n",
        "    training_args.remove_unused_columns = False  # Keep unused columns in dataset\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"qwen2-7b-instruct-trl-sft-pvqa\",  # change this\n",
        "        name=\"qwen2-7b-instruct-trl-sft-pvqa\",  # change this\n",
        "        config=training_args,\n",
        "    )\n",
        "\n",
        "\n",
        "    # model = accelerator.prepare(model)\n",
        "    # train_dataset = accelerator.prepare(train_dataset)\n",
        "    # eval_dataset = accelerator.prepare(eval_dataset)\n",
        "\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=collate_fn,\n",
        "        peft_config=peft_config,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        # accelerator=accelerator,\n",
        "\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "    processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "Ylej7i67tyv6",
        "outputId": "116dec67-e483-4f94-ae41-11aa5294a8f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': 'You are a Vision Language Model specialized in interpreting visual data from medical images.\\n            Focus on delivering accurate, succinct, short answers based on the visual information.\\n            Avoid additional explanation unless absolutely necessary.'}]},\n",
              " {'role': 'user',\n",
              "  'content': [{'type': 'image',\n",
              "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=792x531>},\n",
              "   {'type': 'text', 'text': 'what is present?'}]},\n",
              " {'role': 'assistant', 'content': [{'type': 'text', 'text': 'vasculature'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(sample, system_message):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": sample[\"image\"],\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample['question'],\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[\"answer\"]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "    # Get the texts and images, and apply the chat template\n",
        "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]  # Prepare texts for processing\n",
        "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)  # Encode texts and images into tensors\n",
        "\n",
        "\n",
        "    # for key in batch.keys():\n",
        "    #     batch[key] = batch[key].to(device)\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
        "\n",
        "    # Ignore the image token index in the loss computation (model specific)\n",
        "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
        "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
        "    else:\n",
        "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
        "\n",
        "    # Mask image token IDs in the labels\n",
        "    for image_token_id in image_tokens:\n",
        "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
        "\n",
        "    batch[\"labels\"] = labels  # Add labels to the batch\n",
        "\n",
        "    # print({key: value.device for key, value in batch.items()})\n",
        "\n",
        "    return batch  # Return the prepared batch\n",
        "\n",
        "def sample_subset(dataset_split, n):\n",
        "    return dataset_split.shuffle(seed=42).select(range(min(len(dataset_split), n)))\n",
        "\n",
        "\n",
        "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from medical images.\n",
        "            Focus on delivering accurate, succinct, short answers based on the visual information.\n",
        "            Avoid additional explanation unless absolutely necessary.\"\"\"\n",
        "\n",
        "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
        "n_samples = 6\n",
        "\n",
        "train_subset = sample_subset(dataset[\"train\"], n_samples)\n",
        "\n",
        "# Optional: Save the subsets or print them\n",
        "print(\"Sampled Train Dataset:\", train_subset)\n",
        "\n",
        "train_dataset = [format_data(sample, system_message) for sample in train_subset]\n",
        "batch = collate_fn(train_dataset)"
      ],
      "metadata": {
        "id": "QxDoAMdzvjoL",
        "outputId": "be0646ae-4080-4818-cd26-b5f6102ab85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled Train Dataset: Dataset({\n",
            "    features: ['image', 'question', 'answer'],\n",
            "    num_rows: 6\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'].shape, batch['labels'].shape"
      ],
      "metadata": {
        "id": "vtzY24UXw8tm",
        "outputId": "63b2d87d-35c2-4681-bfd8-614e87177961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 606]), torch.Size([6, 606]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'][0]"
      ],
      "metadata": {
        "id": "fsfUjYZF3ZWs",
        "outputId": "8e01f3a5-5eaa-484f-af26-8e7b4c31f2e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
              "        151643, 151643, 151644,   8948,    198,   2610,    525,    264,  30441,\n",
              "         11434,   4903,  27076,    304,  65644,   9124,    821,    504,   6457,\n",
              "          5335,    624,    310,  25806,    389,  23988,  13382,     11,  98632,\n",
              "            11,   2805,  11253,   3118,    389,    279,   9124,   1995,    624,\n",
              "           310,  34006,   5107,  16148,   7241,  10875,   5871,     13, 151645,\n",
              "           198, 151644,    872,    198, 151652, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
              "        151655, 151655, 151655, 151655, 151655, 151655, 151653,  12555,    374,\n",
              "          3042,     30, 151645,    198, 151644,  77091,    198,   4281,   3314,\n",
              "          1568, 151645,    198])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['labels'][0]"
      ],
      "metadata": {
        "id": "4_2H7eOL3tyO",
        "outputId": "b2868606-289e-4aaa-e4cb-06c51e636c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100, 151644,   8948,    198,   2610,    525,    264,  30441,\n",
              "         11434,   4903,  27076,    304,  65644,   9124,    821,    504,   6457,\n",
              "          5335,    624,    310,  25806,    389,  23988,  13382,     11,  98632,\n",
              "            11,   2805,  11253,   3118,    389,    279,   9124,   1995,    624,\n",
              "           310,  34006,   5107,  16148,   7241,  10875,   5871,     13, 151645,\n",
              "           198, 151644,    872,    198,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
              "          -100,   -100,   -100,   -100,   -100,   -100,   -100,  12555,    374,\n",
              "          3042,     30, 151645,    198, 151644,  77091,    198,   4281,   3314,\n",
              "          1568, 151645,    198])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_K-Yx-g3yzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}